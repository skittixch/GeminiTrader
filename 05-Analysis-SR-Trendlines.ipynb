{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3022cf37-3fec-4284-98ad-9f201598dcf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup, Logging, Configuration\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from decimal import Decimal, getcontext\n",
    "\n",
    "# --- Core Configuration ---\n",
    "SYMBOL = \"BTCUSDT\" # Define the primary symbol we are working with\n",
    "QUOTE_ASSET = 'USDT' # Quote asset for calculations\n",
    "BASE_ASSET = SYMBOL.replace(QUOTE_ASSET, '') # Base asset\n",
    "\n",
    "# --- Precision ---\n",
    "# Set precision for Decimal calculations (adjust as needed, high enough for crypto)\n",
    "getcontext().prec = 18 # Example: 18 decimal places\n",
    "\n",
    "# --- Logging Setup ---\n",
    "# Clear existing root handlers to avoid duplicate logs in Jupyter environments\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    stream=sys.stdout) # Explicitly direct logs to stdout\n",
    "\n",
    "# --- Pandas Display Options ---\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "# CRITICAL: Ensure Decimal/float output is not in scientific notation\n",
    "# Use a lambda function for more robust Decimal formatting if needed\n",
    "pd.set_option('display.float_format', lambda x: f'{Decimal(str(x)):.8f}') # Format as Decimal with 8 places for display\n",
    "\n",
    "logging.info(\"✅ Cell 1 Setup Complete: Imports, Logging, Config.\")\n",
    "\n",
    "# End of Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d467ee-7680-4d9d-8dfe-6e407e02de28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Get Exchange Filters and Define Adjustment Helpers\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from decimal import Decimal, ROUND_DOWN, ROUND_UP\n",
    "import math\n",
    "from binance.client import Client\n",
    "from dotenv import load_dotenv\n",
    "import sys # Import sys here for logging stream\n",
    "\n",
    "# --- Prerequisites ---\n",
    "if 'SYMBOL' not in locals(): raise RuntimeError(\"Run Cell 1 for SYMBOL.\")\n",
    "\n",
    "# --- Client Initialization (Minimal for Filters) ---\\\n",
    "client_filters = None\n",
    "try:\n",
    "    load_dotenv(verbose=False) # Load .env file if it exists\n",
    "    API_KEY_FILTERS = os.environ.get('BINANCE_API_KEY')\n",
    "    SECRET_KEY_FILTERS = os.environ.get('BINANCE_SECRET_KEY') or os.environ.get('BINANCE_API_SECRET')\n",
    "    if API_KEY_FILTERS and SECRET_KEY_FILTERS:\n",
    "        # Increase timeout settings\n",
    "        client_filters = Client(API_KEY_FILTERS, SECRET_KEY_FILTERS, tld='us', requests_params={'timeout': 60})\n",
    "        client_filters.ping() # Test connection\n",
    "        logging.info(\"Minimal client initialized for fetching filters.\")\n",
    "    else:\n",
    "        logging.warning(\"API keys not found in .env, cannot fetch live filters. Will use hardcoded defaults.\")\n",
    "except Exception as e:\n",
    "    logging.warning(f\"Could not initialize client for filters: {e}. Using hardcoded defaults.\")\n",
    "    client_filters = None # Ensure it's None if init fails\n",
    "\n",
    "# --- Fetch/Define Symbol Filters ---\n",
    "price_tick_size_bt = None\n",
    "min_qty_bt = None\n",
    "qty_step_size_bt = None\n",
    "min_notional_bt = Decimal('0') # Default\n",
    "\n",
    "if client_filters:\n",
    "    logging.info(f\"Fetching exchange filters for {SYMBOL}...\")\n",
    "    try:\n",
    "        exchange_info_bt = client_filters.get_exchange_info()\n",
    "        all_symbols_info_bt = exchange_info_bt.get('symbols', [])\n",
    "        symbol_info_lookup_bt = {s['symbol']: s for s in all_symbols_info_bt}\n",
    "        symbol_info_bt = symbol_info_lookup_bt.get(SYMBOL)\n",
    "        if symbol_info_bt:\n",
    "            for f in symbol_info_bt.get('filters',[]):\n",
    "                if f['filterType']=='PRICE_FILTER': price_tick_size_bt=Decimal(f['tickSize'])\n",
    "                elif f['filterType']=='LOT_SIZE': min_qty_bt=Decimal(f['minQty']); qty_step_size_bt=Decimal(f['stepSize'])\n",
    "                elif f['filterType'] == 'MIN_NOTIONAL': min_notional_bt = Decimal(f.get('minNotional', '0')) # Handle potential absence\n",
    "\n",
    "            if not all([price_tick_size_bt is not None, min_qty_bt is not None, qty_step_size_bt is not None, min_notional_bt > 0]):\n",
    "                 logging.warning(f\"Essential filters missing/invalid from API response (Tick:{price_tick_size_bt}, MinQty:{min_qty_bt}, Step:{qty_step_size_bt}, MinNotional:{min_notional_bt}). Falling back to defaults.\")\n",
    "                 client_filters = None # Force fallback\n",
    "            else:\n",
    "                logging.info(f\"Live Filters Fetched: Tick={price_tick_size_bt}, Step={qty_step_size_bt}, MinQty={min_qty_bt}, MinNotional={min_notional_bt}\")\n",
    "        else:\n",
    "             logging.warning(f\"Symbol {SYMBOL} not found in exchange info. Falling back to defaults.\")\n",
    "             client_filters = None # Force fallback\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Failed to get/parse live filters: {e}. Falling back to defaults.\")\n",
    "        client_filters = None # Force fallback\n",
    "\n",
    "# Hardcoded Defaults (Used if client init or fetch fails) - Based on previous BTCUSDT run\n",
    "if not client_filters or not all([price_tick_size_bt is not None, min_qty_bt is not None, qty_step_size_bt is not None, min_notional_bt > 0]):\n",
    "    logging.warning(f\"Using **HARDCODED DEFAULT FILTERS** for BTCUSDT.\")\n",
    "    price_tick_size_bt = Decimal('0.01000000')\n",
    "    min_qty_bt = Decimal('0.00001000')\n",
    "    qty_step_size_bt = Decimal('0.00001000')\n",
    "    min_notional_bt = Decimal('1.00000000') # Common value for BTCUSDT on Binance.US\n",
    "    logging.info(f\"Defaults Used: Tick={price_tick_size_bt}, Step={qty_step_size_bt}, MinQty={min_qty_bt}, MinNotional={min_notional_bt}\")\n",
    "\n",
    "\n",
    "# --- Adjustment Helper Functions ---\n",
    "def adjust_price_bt(price, tick_size):\n",
    "    \"\"\"Adjusts entry price DOWN to the nearest tick size multiple.\"\"\"\n",
    "    try:\n",
    "        if not (isinstance(price, Decimal) and isinstance(tick_size, Decimal) and tick_size > 0):\n",
    "            # logging.warning(f\"Invalid input for adjust_price_bt: price={price}, tick_size={tick_size}\")\n",
    "            return None\n",
    "        adjusted_price = (price // tick_size) * tick_size\n",
    "        # logging.debug(f\"Adjust Price Down: Raw={price}, Tick={tick_size}, Adjusted={adjusted_price}\")\n",
    "        return adjusted_price\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in adjust_price_bt: {e}\")\n",
    "        return None\n",
    "\n",
    "def adjust_qty_bt(quantity, min_q, step_size):\n",
    "    \"\"\"Adjusts quantity DOWN to the nearest step size multiple, ensuring it's >= min_q.\"\"\"\n",
    "    try:\n",
    "        if not (isinstance(quantity,Decimal) and isinstance(min_q,Decimal) and isinstance(step_size,Decimal)):\n",
    "            # logging.warning(f\"Invalid input types for adjust_qty_bt: qty={type(quantity)}, min_q={type(min_q)}, step={type(step_size)}\")\n",
    "            return Decimal('0')\n",
    "        if quantity < min_q:\n",
    "            # logging.debug(f\"Adjust Qty Down: Raw Qty {quantity} < Min Qty {min_q}. Returning 0.\")\n",
    "            return Decimal('0') # Cannot place order if raw quantity is already less than minimum\n",
    "        if step_size <= 0:\n",
    "            # logging.warning(f\"Invalid step_size for adjust_qty_bt: {step_size}\")\n",
    "            return Decimal('0')\n",
    "\n",
    "        # Calculate the number of steps, rounding down\n",
    "        num_steps = math.floor(quantity / step_size)\n",
    "        adjusted_qty = num_steps * step_size\n",
    "\n",
    "        # Final check: Ensure adjusted quantity is still >= minimum quantity\n",
    "        if adjusted_qty < min_q:\n",
    "            # This should rarely happen if initial check passed, but handles edge cases\n",
    "            # logging.debug(f\"Adjust Qty Down: Adjusted Qty {adjusted_qty} < Min Qty {min_q}. Returning 0.\")\n",
    "            return Decimal('0')\n",
    "\n",
    "        # logging.debug(f\"Adjust Qty Down: Raw={quantity}, MinQ={min_q}, Step={step_size}, Adjusted={adjusted_qty}\")\n",
    "        return adjusted_qty\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in adjust_qty_bt: {e}\")\n",
    "        return Decimal('0')\n",
    "\n",
    "def adjust_tp_price_bt(price, tick_size):\n",
    "    \"\"\" Adjusts TP price UP to the nearest tick size multiple. \"\"\"\n",
    "    try:\n",
    "        if not (isinstance(price, Decimal) and isinstance(tick_size, Decimal) and tick_size > 0):\n",
    "             # logging.warning(f\"Invalid input for adjust_tp_price_bt: price={price}, tick_size={tick_size}\")\n",
    "             return None\n",
    "        # Calculate ticks, round UP using ceiling division logic, then multiply back\n",
    "        ticks = price / tick_size\n",
    "        adjusted_ticks = ticks.to_integral_value(rounding=ROUND_UP) # Round UP to nearest integer number of ticks\n",
    "        adjusted_price = adjusted_ticks * tick_size\n",
    "        # logging.debug(f\"Adjust TP Price UP: Raw={price}, Tick={tick_size}, Adjusted={adjusted_price}\")\n",
    "        return adjusted_price\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in adjust_tp_price_bt: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Log the final values being used\n",
    "logging.info(f\"Using Filters: Tick={price_tick_size_bt}, Step={qty_step_size_bt}, MinQty={min_qty_bt}, MinNotional={min_notional_bt}\")\n",
    "logging.info(\"✅ Cell 3 Complete: Filters and helper functions ready.\")\n",
    "\n",
    "# End of Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0241c-d830-4561-8872-b77a02a367e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Fetch Multi-Timeframe Data & Calculate Indicators\n",
    "\n",
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import logging\n",
    "import os\n",
    "from binance.client import Client\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import time # For potential retries\n",
    "\n",
    "# --- Prerequisites ---\n",
    "if 'SYMBOL' not in locals(): raise RuntimeError(\"Run Cell 1 for SYMBOL.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "TIMEFRAMES = {\n",
    "    '1d': Client.KLINE_INTERVAL_1DAY,\n",
    "    '4h': Client.KLINE_INTERVAL_4HOUR,\n",
    "    '1h': Client.KLINE_INTERVAL_1HOUR\n",
    "}\n",
    "# Use consistent date range from original backtest data fetch\n",
    "DATE_START = \"1 Jan, 2023\"\n",
    "DATE_END = \"1 Jan, 2024\" # get_historical_klines is exclusive of end date for daily+\n",
    "\n",
    "KLINE_COLUMNS = [\n",
    "    'Open Time', 'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "    'Close Time', 'Quote Asset Volume', 'Number of Trades',\n",
    "    'Taker Buy Base Asset Volume', 'Taker Buy Quote Asset Volume', 'Ignore'\n",
    "]\n",
    "DECIMAL_COLS = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "FINAL_COLS = ['Open', 'High', 'Low', 'Close', 'Volume'] # Keep only these for base DFs\n",
    "ATR_PERIOD = 14 # Standard ATR period\n",
    "\n",
    "# --- Client Initialization ---\n",
    "client_data = None\n",
    "try:\n",
    "    load_dotenv(verbose=False)\n",
    "    API_KEY_DATA = os.environ.get('BINANCE_API_KEY')\n",
    "    SECRET_KEY_DATA = os.environ.get('BINANCE_SECRET_KEY') or os.environ.get('BINANCE_API_SECRET')\n",
    "    if API_KEY_DATA and SECRET_KEY_DATA:\n",
    "        # Increase timeout settings\n",
    "        client_data = Client(API_KEY_DATA, SECRET_KEY_DATA, tld='us', requests_params={'timeout': 60})\n",
    "        client_data.ping() # Test connection\n",
    "        logging.info(\"Binance client initialized successfully for historical data.\")\n",
    "    else:\n",
    "        logging.error(\"API keys not found, cannot fetch historical data.\")\n",
    "        raise RuntimeError(\"API keys missing.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Could not initialize client for data fetch: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Data Fetching and Processing ---\n",
    "data_for_sr_calc = {} # Dictionary to hold original DFs: {'1h': df_1h, '4h': df_4h, ...}\n",
    "all_tf_data_processed = {} # Dictionary for DFs with indicators\n",
    "required_cols_for_loop = ['Open', 'High', 'Low', 'Close', 'Volume'] # Base columns for backtest loop df\n",
    "\n",
    "logging.info(f\"--- Fetching Multi-Timeframe Data for {SYMBOL} ({DATE_START} to {DATE_END}) ---\")\n",
    "\n",
    "# Use pandas_ta if available for ATR\n",
    "try:\n",
    "    import pandas_ta as ta\n",
    "    use_pandas_ta = True\n",
    "    logging.info(\"Using pandas_ta for ATR calculation.\")\n",
    "except ImportError:\n",
    "    use_pandas_ta = False\n",
    "    logging.warning(\"pandas_ta not found. ATR calculation will be skipped.\")\n",
    "    # Potentially add manual ATR calculation here if needed as fallback\n",
    "\n",
    "for tf_key, tf_interval in TIMEFRAMES.items():\n",
    "    logging.info(f\"Fetching {tf_key} data...\")\n",
    "    retries = 3\n",
    "    klines_raw = None\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            klines_raw = client_data.get_historical_klines(\n",
    "                SYMBOL, tf_interval, DATE_START, DATE_END\n",
    "            )\n",
    "            break # Success\n",
    "        except Exception as e:\n",
    "            retries -= 1\n",
    "            logging.warning(f\"Error fetching {tf_key} data: {e}. Retries left: {retries}\")\n",
    "            if retries == 0:\n",
    "                logging.error(f\"❌ Failed to fetch {tf_key} data after multiple attempts.\")\n",
    "                klines_raw = None # Ensure it's None if all retries fail\n",
    "            else:\n",
    "                time.sleep(2) # Wait before retrying\n",
    "\n",
    "    if not klines_raw:\n",
    "         logging.warning(f\"No {tf_key} data returned for {SYMBOL} in the specified range.\")\n",
    "         data_for_sr_calc[tf_key] = pd.DataFrame(columns=FINAL_COLS)\n",
    "         all_tf_data_processed[tf_key] = pd.DataFrame(columns=FINAL_COLS + [f'ATR_{ATR_PERIOD}']) # Include ATR col even if empty\n",
    "         continue\n",
    "\n",
    "    df = pd.DataFrame(klines_raw, columns=KLINE_COLUMNS)\n",
    "    df['Open Time'] = pd.to_datetime(df['Open Time'], unit='ms', utc=True)\n",
    "    df.set_index('Open Time', inplace=True)\n",
    "    for col in DECIMAL_COLS:\n",
    "         df[col] = df[col].apply(lambda x: Decimal(str(x)))\n",
    "\n",
    "    # Store the raw-ish data for dynamic S/R calculations\n",
    "    data_for_sr_calc[tf_key] = df[FINAL_COLS].copy()\n",
    "\n",
    "    # Calculate ATR\n",
    "    atr_col_name = f'ATR_{ATR_PERIOD}'\n",
    "    if use_pandas_ta:\n",
    "        try:\n",
    "            # Ensure correct types for pandas_ta\n",
    "            high_f = df['High'].astype(float)\n",
    "            low_f = df['Low'].astype(float)\n",
    "            close_f = df['Close'].astype(float)\n",
    "            df[atr_col_name] = ta.atr(high=high_f, low=low_f, close=close_f, length=ATR_PERIOD)\n",
    "            df[atr_col_name] = df[atr_col_name].apply(lambda x: Decimal(str(x)) if pd.notna(x) else None)\n",
    "            logging.info(f\"  Calculated ATR for {tf_key}.\")\n",
    "        except Exception as e_atr:\n",
    "            logging.error(f\"  Error calculating ATR for {tf_key} using pandas_ta: {e_atr}\")\n",
    "            df[atr_col_name] = None # Set to None on error\n",
    "    else:\n",
    "        df[atr_col_name] = None # Set to None if pandas_ta not available\n",
    "\n",
    "    all_tf_data_processed[tf_key] = df[FINAL_COLS + [atr_col_name]].copy()\n",
    "    if atr_col_name not in required_cols_for_loop:\n",
    "        required_cols_for_loop.append(atr_col_name)\n",
    "    logging.info(f\"✅ Successfully processed {len(df)} {tf_key} klines. Range: {df.index.min()} to {df.index.max()}\")\n",
    "\n",
    "\n",
    "# --- Align Data to 1H Index & Create Backtest DataFrame ---\n",
    "logging.info(\"Aligning multi-timeframe data (with ATR) to 1H index...\")\n",
    "if '1h' not in all_tf_data_processed or all_tf_data_processed['1h'].empty:\n",
    "    raise RuntimeError(\"1H data is missing or empty, cannot align for backtest.\")\n",
    "\n",
    "df_1h_processed = all_tf_data_processed['1h'].copy()\n",
    "aligned_dfs = [df_1h_processed]\n",
    "\n",
    "for tf_key in ['4h', '1d']:\n",
    "    if tf_key in all_tf_data_processed and not all_tf_data_processed[tf_key].empty:\n",
    "        df_aligned = all_tf_data_processed[tf_key].reindex(df_1h_processed.index, method='ffill').add_suffix(f'_{tf_key}')\n",
    "        aligned_dfs.append(df_aligned)\n",
    "        # Add the suffixed ATR column name to required columns for the loop data\n",
    "        atr_col_tf = f'ATR_{ATR_PERIOD}_{tf_key}'\n",
    "        if atr_col_tf not in required_cols_for_loop:\n",
    "            required_cols_for_loop.append(atr_col_tf)\n",
    "    else:\n",
    "        logging.warning(f\"{tf_key} data is missing or empty, cannot include in alignment.\")\n",
    "\n",
    "# Combine aligned data\n",
    "historical_data_test = pd.concat(aligned_dfs, axis=1)\n",
    "\n",
    "# Select only the columns genuinely needed for the backtest loop logic\n",
    "# Remove duplicates just in case, keeping the first occurrence (should be the 1h one)\n",
    "final_loop_cols = list(dict.fromkeys(required_cols_for_loop))\n",
    "\n",
    "# Ensure all required columns actually exist after alignment and suffixing\n",
    "missing_cols = [col for col in final_loop_cols if col not in historical_data_test.columns]\n",
    "if missing_cols:\n",
    "    logging.warning(f\"Columns missing after alignment: {missing_cols}. Removing them from loop list.\")\n",
    "    final_loop_cols = [col for col in final_loop_cols if col not in missing_cols]\n",
    "\n",
    "# Drop rows with any NaNs in the essential columns (especially initial ATR NaNs)\n",
    "rows_before_dropna = len(historical_data_test)\n",
    "historical_data_test = historical_data_test[final_loop_cols].dropna().copy()\n",
    "rows_after_dropna = len(historical_data_test)\n",
    "logging.info(f\"Data aligned. Dropped {rows_before_dropna - rows_after_dropna} rows due to NaNs (e.g., initial ATR).\")\n",
    "logging.info(f\"Backtest will run on {rows_after_dropna} 1H candles.\")\n",
    "\n",
    "if historical_data_test.empty:\n",
    "    raise ValueError(\"No data left after dropping NaNs from ATR/Alignment. Cannot proceed.\")\n",
    "\n",
    "# Create the specific TF variables for convenience if needed elsewhere (though data_for_sr_calc is primary)\n",
    "df_1h = data_for_sr_calc.get('1h', pd.DataFrame())\n",
    "df_4h = data_for_sr_calc.get('4h', pd.DataFrame())\n",
    "df_1d = data_for_sr_calc.get('1d', pd.DataFrame())\n",
    "\n",
    "logging.info(\"✅ Cell 2 Complete: Multi-TF Data Fetched, Indicators Calculated, Aligned for Backtest.\")\n",
    "print(\"\\n--- Sample of Aligned Backtest Data (historical_data_test) ---\")\n",
    "print(historical_data_test.head().to_string())\n",
    "print(\"\\n--- Sample of Raw Data for S/R Calc (data_for_sr_calc['1h']) ---\")\n",
    "print(data_for_sr_calc['1h'].head().to_string())\n",
    "\n",
    "\n",
    "# End of Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2485a9-38cf-4b44-9174-949812bfc6d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Dynamic S/R Zone Calculation Helper Function (SIMPLIFIED DEFINITION FOR DEBUG)\n",
    "\n",
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# --- Logging Setup (Basic fallback) ---\n",
    "if not logging.getLogger().hasHandlers():\n",
    "     logging.basicConfig(level=logging.INFO,\n",
    "                         format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                         stream=sys.stdout)\n",
    "\n",
    "# --- SIMPLIFIED Helper Function Definition ---\n",
    "def calculate_dynamic_zones(data_slice, n_periods, sup_thresh_pct, res_thresh_pct):\n",
    "    \"\"\"\n",
    "    SIMPLIFIED VERSION FOR DEBUGGING: Returns empty DataFrames.\n",
    "    \"\"\"\n",
    "    logging.debug(\"SIMPLIFIED calculate_dynamic_zones called.\") # Log if called\n",
    "    # Return empty dataframes matching the expected structure\n",
    "    empty_df = pd.DataFrame(columns=['zone_min_price', 'zone_max_price', 'points', 'timestamps', 'num_points'])\n",
    "    return empty_df.copy(), empty_df.copy()\n",
    "\n",
    "# *** ADDED DEBUG PRINT HERE ***\n",
    "print(\"<<<<< DEBUG: calculate_dynamic_zones SIMPLIFIED FUNCTION DEFINITION COMPLETE >>>>>\")\n",
    "# *** END ADDITION ***\n",
    "\n",
    "logging.info(\"✅ Cell 4 Complete: Dynamic S/R Zone calculation function defined (SIMPLIFIED).\")\n",
    "\n",
    "# End of Cell 4 (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021dee9-e7ae-4002-80fc-5680dc7a35e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5 (Revised v8 - Check This is Correct): Replicate Bitcoin Long-Term Power Law - Adjusted X-Axis Limit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy import stats # For linear regression\n",
    "from datetime import datetime, timedelta, timezone # Ensure timezone is imported\n",
    "import os\n",
    "from decimal import Decimal # Ensure Decimal is imported\n",
    "\n",
    "# Install yfinance if needed\n",
    "# !pip install yfinance scipy pandas_ta matplotlib numpy\n",
    "\n",
    "import yfinance as yf\n",
    "try:\n",
    "    import pandas_ta as ta\n",
    "    # logging.info(\"Successfully imported pandas_ta.\") # Keep logging less verbose\n",
    "except ImportError:\n",
    "    logging.warning(\"pandas_ta not found, but not required for this cell.\")\n",
    "\n",
    "# --- Configuration ---\\\n",
    "TICKER_PL = \"BTC-USD\"\n",
    "INTERVAL_PL = \"1d\"\n",
    "# Use a very early start date to get max history from yfinance\n",
    "# yfinance will automatically adjust if data isn't available that far back\n",
    "START_DATE_PL = \"2010-07-17\"\n",
    "END_DATE_PL = datetime.now().strftime('%Y-%m-%d')\n",
    "FORECAST_YEARS = 20 # Keep calculating the full forecast in df_lines\n",
    "PLOT_FUTURE_YEARS = 2 # How many years past the last data point to show on the plot\n",
    "\n",
    "# --- Logging & Display Setup ---\n",
    "# Ensure logging is configured (should be by Cell 1, but safeguard)\n",
    "if not logging.getLogger().hasHandlers():\n",
    "     logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', stream=sys.stdout)\n",
    "# Set display options\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "# ---\n",
    "\n",
    "# --- Fetch Data ---\\\n",
    "logging.info(f\"--- Fetching Long-Term Daily Data for {TICKER_PL} for Power Law ({START_DATE_PL} to {END_DATE_PL}) ---\")\n",
    "df_daily_pl_raw = pd.DataFrame()\n",
    "close_prices_series_pl = pd.Series(dtype=float) # Initialize empty Series\n",
    "\n",
    "try:\n",
    "    # Use auto_adjust=True for simplicity with yfinance daily data\n",
    "    df_fetched_pl = yf.download(TICKER_PL, start=START_DATE_PL, end=END_DATE_PL, interval=INTERVAL_PL, progress=False, auto_adjust=True)\n",
    "    if df_fetched_pl.empty: raise ValueError(\"No data returned from yfinance for Power Law.\")\n",
    "    logging.info(f\"✅ Successfully fetched {len(df_fetched_pl)} daily data points for PL.\")\n",
    "    logging.info(f\"PL Data range: {df_fetched_pl.index.min()} to {df_fetched_pl.index.max()}\")\n",
    "\n",
    "    # Robustly select 'Close' column AND ensure it's a Series\n",
    "    if 'Close' not in df_fetched_pl.columns: raise ValueError(f\"'Close' column not found in PL data. Columns: {df_fetched_pl.columns}\")\n",
    "    close_data_pl = df_fetched_pl['Close']\n",
    "    if isinstance(close_data_pl, pd.DataFrame):\n",
    "        if not close_data_pl.empty: close_prices_series_pl = close_data_pl.iloc[:, 0].copy()\n",
    "        else: raise ValueError(\"Selected 'Close' (PL) resulted in an empty DataFrame.\")\n",
    "    elif isinstance(close_data_pl, pd.Series): close_prices_series_pl = close_data_pl.copy()\n",
    "    else: raise TypeError(f\"Unexpected data type after selecting 'Close' (PL): {type(close_data_pl)}\")\n",
    "\n",
    "    if not isinstance(close_prices_series_pl, pd.Series): raise TypeError(f\"Could not extract Close (PL) as a Pandas Series. Final type was: {type(close_prices_series_pl)}\")\n",
    "\n",
    "    # Store in df_daily_pl_raw and clean\n",
    "    df_daily_pl_raw = pd.DataFrame({'Close': close_prices_series_pl})\n",
    "    df_daily_pl_raw.dropna(inplace=True)\n",
    "    # Convert 'Close' to Decimal AFTER initial processing and NaN drop\n",
    "    df_daily_pl_raw['Close'] = df_daily_pl_raw['Close'].apply(lambda x: Decimal(str(x)))\n",
    "    df_daily_pl_raw = df_daily_pl_raw[df_daily_pl_raw['Close'] > 0] # Filter for positive prices\n",
    "\n",
    "    if df_daily_pl_raw.empty: raise ValueError(\"No positive Close price data remaining after cleaning for PL.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"❌ Failed to download or process data for Power Law: {e}\", exc_info=True)\n",
    "    # Ensure df_lines is not created or is empty on failure\n",
    "    df_lines = pd.DataFrame() # Explicitly clear df_lines on error\n",
    "    raise # Re-raise the error to stop execution if PL calc fails\n",
    "\n",
    "\n",
    "# --- Prepare Data for Log-Linear Regression (Only if data fetch succeeded) ---\\\n",
    "if not df_daily_pl_raw.empty:\n",
    "    try:\n",
    "        # Ensure index is DatetimeIndex and localized to UTC\n",
    "        if not isinstance(df_daily_pl_raw.index, pd.DatetimeIndex):\n",
    "            raise TypeError(\"Index is not DatetimeIndex after fetching PL data.\")\n",
    "        if df_daily_pl_raw.index.tz is None:\n",
    "            df_daily_pl_raw.index = df_daily_pl_raw.index.tz_localize('UTC')\n",
    "        elif df_daily_pl_raw.index.tz != timezone.utc:\n",
    "            df_daily_pl_raw.index = df_daily_pl_raw.index.tz_convert('UTC')\n",
    "\n",
    "        actual_start_date_pl = df_daily_pl_raw.index[0]\n",
    "        start_num_date_pl = mdates.date2num(actual_start_date_pl)\n",
    "\n",
    "        # Convert Days and Log_Close to float for regression\n",
    "        df_daily_pl_raw['Days_float'] = (df_daily_pl_raw.index - actual_start_date_pl).days.astype(float)\n",
    "        df_daily_pl_raw['Log_Close_float'] = df_daily_pl_raw['Close'].apply(lambda x: np.log(float(x)))\n",
    "\n",
    "        # --- Perform Linear Regression ---\\\n",
    "        logging.info(\"Performing log-linear regression...\")\n",
    "        df_reg = df_daily_pl_raw[['Days_float', 'Log_Close_float']].dropna()\n",
    "        if df_reg.empty: raise ValueError(\"No valid data points for regression.\")\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(df_reg['Days_float'], df_reg['Log_Close_float'])\n",
    "        logging.info(f\"Regression Results: Slope={slope:.6f}, Intercept={intercept:.4f}, R^2={r_value**2:.4f}\")\n",
    "\n",
    "        # --- Calculate Regression, Support, Resistance Lines (Historical Min/Max Deviation) ---\\\n",
    "        last_date = df_daily_pl_raw.index[-1]\n",
    "        # Calculate future dates for the full forecast period\n",
    "        future_dates = pd.date_range(start=last_date + timedelta(days=1), periods=365 * FORECAST_YEARS, freq='D', tz='UTC')\n",
    "        all_dates = df_daily_pl_raw.index.union(future_dates) # Index for df_lines includes full forecast\n",
    "\n",
    "        # Use numeric days directly from the 'Days_float' column where available, calculate for future\n",
    "        all_days_numeric_float = np.array([(d - actual_start_date_pl).days for d in all_dates], dtype=float)\n",
    "\n",
    "        log_reg_line_float = intercept + slope * all_days_numeric_float\n",
    "        log_reg_historical_float = intercept + slope * df_reg['Days_float']\n",
    "        residuals_float = df_reg['Log_Close_float'] - log_reg_historical_float\n",
    "        log_offset_resistance_float = residuals_float.max()\n",
    "        log_offset_support_float = residuals_float.min()\n",
    "        logging.info(f\"Calculated Log Offsets (float): Support={log_offset_support_float:.4f}, Resistance={log_offset_resistance_float:.4f}\")\n",
    "        log_support_line_float = log_reg_line_float + log_offset_support_float\n",
    "        log_resistance_line_float = log_reg_line_float + log_offset_resistance_float\n",
    "\n",
    "        # --- Convert Lines Back to Price Scale (Store as Decimal) ---\\\n",
    "        # Important: Exponentiation can lead to very large numbers\n",
    "        with np.errstate(over='raise'): # Raise error on overflow during exp\n",
    "             try:\n",
    "                 reg_line_price_float = np.exp(log_reg_line_float)\n",
    "                 support_line_price_float = np.exp(log_support_line_float)\n",
    "                 resistance_line_price_float = np.exp(log_resistance_line_float)\n",
    "             except FloatingPointError as fpe:\n",
    "                  logging.error(f\"Overflow during np.exp calculation: {fpe}\")\n",
    "                  raise ValueError(\"Overflow calculating PL lines.\") from fpe\n",
    "\n",
    "        df_lines = pd.DataFrame(index=all_dates) # df_lines still covers the full forecast\n",
    "        df_lines['Regression'] = [Decimal(str(p)) for p in reg_line_price_float]\n",
    "        df_lines['Support'] = [Decimal(str(p)) for p in support_line_price_float]\n",
    "        df_lines['Resistance'] = [Decimal(str(p)) for p in resistance_line_price_float]\n",
    "\n",
    "        # --- Ensure df_lines Index is Timezone-Aware (UTC) ---\n",
    "        # This should be guaranteed by index creation/union, but double-check\n",
    "        if df_lines.index.tz is None or df_lines.index.tz != timezone.utc:\n",
    "             logging.warning(\"df_lines index needs UTC conversion/localization.\")\n",
    "             if df_lines.index.tz is None: df_lines.index = df_lines.index.tz_localize('UTC')\n",
    "             else: df_lines.index = df_lines.index.tz_convert('UTC')\n",
    "\n",
    "        logging.info(\"✅ df_lines DataFrame created successfully.\")\n",
    "\n",
    "        # --- Plotting ---\\\n",
    "        logging.info(f\"Plotting results (Historical Min/Max Channel)...\\\")\")\n",
    "        fig, ax = plt.subplots(figsize=(15, 8))\n",
    "        # Plot using float for consistency with lines\n",
    "        ax.semilogy(df_daily_pl_raw.index, df_daily_pl_raw['Close'].astype(float), label=f'{TICKER_PL} Close Price', color='orange', linewidth=1, alpha=0.8)\n",
    "        ax.semilogy(df_lines.index, df_lines['Regression'].astype(float), label='Log-Linear Regression Fit', color='green', linestyle='-', linewidth=1.5)\n",
    "        ax.semilogy(df_lines.index, df_lines['Support'].astype(float), label='Support (Historical Min Deviation)', color='red', linestyle='-', linewidth=1.5)\n",
    "        ax.semilogy(df_lines.index, df_lines['Resistance'].astype(float), label='Resistance (Historical Max Deviation)', color='purple', linestyle='-', linewidth=1.5)\n",
    "        ax.fill_between(df_lines.index, df_lines['Support'].astype(float), df_lines['Resistance'].astype(float), color='grey', alpha=0.1, label='Regression Channel')\n",
    "\n",
    "        # --- Formatting ---\\\n",
    "        ax.set_title(f'{TICKER_PL} Long Term Price (Log Scale) with Historical Min/Max Deviation Channel')\n",
    "        ax.set_xlabel('Year'); ax.set_ylabel('Price (USD) - Log Scale')\n",
    "        ax.grid(True, which=\"both\", linestyle='--', alpha=0.4); ax.legend(loc='lower right')\n",
    "\n",
    "        # Robust Y-Limit Setting\n",
    "        min_val_calc = 0.01; max_val_calc = 10000000 # Defaults\n",
    "        try:\n",
    "            min_close_price = float(df_daily_pl_raw['Close'].min())\n",
    "            max_close_price = float(df_daily_pl_raw['Close'].max())\n",
    "            min_val_calc = max(0.01, min_close_price * 0.5)\n",
    "            plot_end_date = last_date + pd.DateOffset(years=PLOT_FUTURE_YEARS)\n",
    "            # Ensure plot_end_date exists in df_lines index before accessing\n",
    "            if plot_end_date <= df_lines.index[-1]:\n",
    "                 plot_end_date_actual = df_lines.index.asof(plot_end_date) # Find closest valid index\n",
    "                 if pd.notna(plot_end_date_actual):\n",
    "                     max_plot_res = float(df_lines.loc[plot_end_date_actual, 'Resistance'])\n",
    "                 else: # If asof somehow fails, fallback\n",
    "                     max_plot_res = float(df_lines['Resistance'].iloc[-1])\n",
    "            else: # If plot date is beyond calculated forecast, use last forecast value\n",
    "                 max_plot_res = float(df_lines['Resistance'].iloc[-1])\n",
    "            max_val_calc = max(max_close_price * 1.2, max_plot_res * 1.2)\n",
    "        except Exception as e_ylim:\n",
    "             logging.warning(f\"Could not reliably determine ylim: {e_ylim}. Using defaults.\")\n",
    "        ax.set_ylim(bottom=min_val_calc, top=max_val_calc)\n",
    "\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator(2)); ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        ax.xaxis.set_minor_locator(mdates.YearLocator(1)); fig.autofmt_xdate()\n",
    "\n",
    "        # Set X-Axis Limit\n",
    "        plot_right_limit = last_date + pd.DateOffset(years=PLOT_FUTURE_YEARS)\n",
    "        # Ensure left limit is valid Datetime\n",
    "        if isinstance(actual_start_date_pl, pd.Timestamp):\n",
    "             ax.set_xlim(left=actual_start_date_pl, right=plot_right_limit)\n",
    "             logging.info(f\"Plot X-axis range set from {actual_start_date_pl.date()} to {plot_right_limit.date()}\")\n",
    "        else:\n",
    "             logging.warning(\"Could not set xlim left boundary - actual_start_date_pl invalid.\")\n",
    "\n",
    "\n",
    "        plt.tight_layout(); plt.show()\n",
    "        logging.info(\"✅ Power Law concept plot generated (Adjusted X-axis).\")\n",
    "\n",
    "    except Exception as e_process:\n",
    "        logging.error(f\"❌ Failed during PL processing/regression/plotting: {e_process}\", exc_info=True)\n",
    "        # Ensure df_lines is empty if processing fails after fetch\n",
    "        df_lines = pd.DataFrame()\n",
    "        # Don't raise here, allow notebook to continue, but backtester will fail\n",
    "\n",
    "\n",
    "# Safety check: define df_lines as empty if it wasn't created due to prior error\n",
    "if 'df_lines' not in locals():\n",
    "     logging.error(\"df_lines was not created due to errors in fetching or processing PL data.\")\n",
    "     df_lines = pd.DataFrame()\n",
    "\n",
    "\n",
    "# End of Cell 5 (Revised v8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3904993-7b90-4b06-a577-f01eac1a23aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5.1 (v1.1 - Optimized DataFrame Construction): Calculate Logarithmic Rainbow Bands\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "from decimal import Decimal, getcontext\n",
    "\n",
    "# --- Prerequisites ---\n",
    "if 'df_lines' not in locals() or df_lines.empty:\n",
    "    raise RuntimeError(\"df_lines is missing or empty. Run Cell 5 (Power Law) first.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "NUM_RAINBOW_BANDS = 100 # Number of zones/bands (MATCH v13.1 config)\n",
    "\n",
    "# --- Logging ---\n",
    "logger_rb = logging.getLogger(__name__ + \"_rainbow\")\n",
    "if not logging.getLogger().hasHandlers():\n",
    "     logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', stream=sys.stdout)\n",
    "     logging.getLogger().setLevel(logging.INFO)\n",
    "else:\n",
    "     logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# --- Calculation ---\n",
    "logger_rb.info(f\"--- Calculating {NUM_RAINBOW_BANDS} Logarithmic Rainbow Bands (Optimized) ---\")\n",
    "# Initialize rainbow_bands_df as empty for now, will construct at the end\n",
    "rainbow_bands_df = pd.DataFrame()\n",
    "bands_data = {} # Dictionary to hold the calculated band Series\n",
    "\n",
    "try:\n",
    "    # Ensure input columns are Decimal\n",
    "    pl_support = df_lines['Support'].apply(Decimal)\n",
    "    pl_resistance = df_lines['Resistance'].apply(Decimal)\n",
    "\n",
    "    # Calculate log values\n",
    "    log_support = pl_support.apply(lambda x: x.ln() if x > 0 else Decimal('-Infinity'))\n",
    "    log_resistance = pl_resistance.apply(lambda x: x.ln() if x > 0 else Decimal('-Infinity'))\n",
    "\n",
    "    # Calculate the total log height\n",
    "    log_height = log_resistance - log_support\n",
    "\n",
    "    # Handle invalid heights\n",
    "    invalid_height_mask = log_height <= 0\n",
    "    if invalid_height_mask.any():\n",
    "        invalid_height_dates = log_height[invalid_height_mask].index.strftime('%Y-%m-%d').tolist()\n",
    "        logger_rb.warning(f\"Invalid non-positive log channel height found on {len(invalid_height_dates)} dates (e.g., {invalid_height_dates[:5]}). Results may be incorrect for these dates.\")\n",
    "        log_height[invalid_height_mask] = Decimal('NaN')\n",
    "\n",
    "\n",
    "    # Calculate the boundaries for each band and store in dictionary\n",
    "    for i in range(NUM_RAINBOW_BANDS + 1):\n",
    "        band_pct = Decimal(i) / Decimal(NUM_RAINBOW_BANDS)\n",
    "        log_boundary = log_support + (log_height * band_pct)\n",
    "        # Convert back to price scale (handle potential exp(infinity) or exp(NaN))\n",
    "        # This calculation produces a pandas Series for each band\n",
    "        boundary_price_series = log_boundary.apply(lambda x: x.exp() if pd.notna(x) and x != Decimal('-Infinity') else Decimal('NaN'))\n",
    "        # Add the Series to the dictionary\n",
    "        bands_data[f'Band_{i}'] = boundary_price_series\n",
    "\n",
    "    # *** OPTIMIZATION: Construct DataFrame from dictionary ***\n",
    "    rainbow_bands_df = pd.DataFrame(bands_data, index=df_lines.index)\n",
    "    # *** END OPTIMIZATION ***\n",
    "\n",
    "    logger_rb.info(f\"✅ Rainbow bands calculated. Shape: {rainbow_bands_df.shape}. Columns: Band_0...Band_{NUM_RAINBOW_BANDS}\")\n",
    "    print(\"\\n--- Sample Rainbow Band Boundaries (Tail) ---\")\n",
    "    print(rainbow_bands_df.tail(5).to_string(float_format='{:,.2f}'.format))\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logger_rb.error(f\"❌ Error calculating rainbow bands: {e}\", exc_info=True)\n",
    "    rainbow_bands_df = pd.DataFrame() # Ensure it's empty on error\n",
    "\n",
    "\n",
    "# --- Helper Function to Get Band Index (Defined Here - unchanged) ---\n",
    "def get_band_index(price: Decimal, timestamp, bands_df: pd.DataFrame, num_bands: int) -> int:\n",
    "    \"\"\"\n",
    "    Determines the index of the rainbow band a given price falls into at a specific time.\n",
    "    Returns band index (0 to num_bands-1), or -1 if price is outside channel or error.\n",
    "    \"\"\"\n",
    "    if not isinstance(price, Decimal) or price <= 0: return -1 # Invalid price input\n",
    "    try:\n",
    "        bands_row_index = bands_df.index.asof(timestamp)\n",
    "        if pd.isna(bands_row_index): return -1\n",
    "        band_boundaries = bands_df.loc[bands_row_index]\n",
    "        if band_boundaries.isnull().any(): return -1\n",
    "        for i in range(num_bands):\n",
    "            lower_bound = band_boundaries[f'Band_{i}']\n",
    "            upper_bound = band_boundaries[f'Band_{i+1}']\n",
    "            if pd.isna(lower_bound) or pd.isna(upper_bound): continue\n",
    "            if lower_bound <= price < upper_bound: return i\n",
    "        highest_band_boundary_price = band_boundaries[f'Band_{num_bands}']\n",
    "        if pd.notna(highest_band_boundary_price) and price == highest_band_boundary_price: return num_bands - 1\n",
    "        lowest_band_boundary_price = band_boundaries['Band_0']\n",
    "        if pd.notna(lowest_band_boundary_price) and price < lowest_band_boundary_price: return -1\n",
    "        elif pd.notna(highest_band_boundary_price) and price > highest_band_boundary_price: return num_bands - 1\n",
    "    except KeyError as ke: return -1\n",
    "    except Exception as e: logger_rb.error(f\"Error getting band index for price {price} at {timestamp}: {e}\", exc_info=False); return -1\n",
    "    return -1 # Default return\n",
    "\n",
    "# Test the function (optional - unchanged)\n",
    "if not rainbow_bands_df.empty and len(rainbow_bands_df) > 10:\n",
    "    try:\n",
    "        test_ts = rainbow_bands_df.dropna().index[-10]\n",
    "        test_price_low = rainbow_bands_df.loc[test_ts, 'Band_1']\n",
    "        test_price_mid = (rainbow_bands_df.loc[test_ts, f'Band_{NUM_RAINBOW_BANDS//2}'] + rainbow_bands_df.loc[test_ts, f'Band_{NUM_RAINBOW_BANDS//2 + 1}']) / 2\n",
    "        test_price_high = rainbow_bands_df.loc[test_ts, f'Band_{NUM_RAINBOW_BANDS-1}']\n",
    "        test_index_low = get_band_index(test_price_low, test_ts, rainbow_bands_df, NUM_RAINBOW_BANDS)\n",
    "        test_index_mid = get_band_index(test_price_mid, test_ts, rainbow_bands_df, NUM_RAINBOW_BANDS)\n",
    "        test_index_high = get_band_index(test_price_high, test_ts, rainbow_bands_df, NUM_RAINBOW_BANDS)\n",
    "        logger_rb.info(f\"Test Band Index: Low Price ({test_price_low:.2f}) -> Band {test_index_low}\")\n",
    "        logger_rb.info(f\"Test Band Index: Mid Price ({test_price_mid:.2f}) -> Band {test_index_mid}\")\n",
    "        logger_rb.info(f\"Test Band Index: High Price ({test_price_high:.2f}) -> Band {test_index_high}\")\n",
    "    except Exception as test_e:\n",
    "        logger_rb.error(f\"Error during band index test: {test_e}\")\n",
    "\n",
    "\n",
    "# End of Cell 5.1 (v1.1 - Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81da1a6-1149-42ad-af6c-62f13cb6877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 (v13.7.4 - Quieter Fills): Backtesting Engine\n",
    "\n",
    "import pandas as pd\n",
    "from decimal import Decimal, ROUND_DOWN, ROUND_UP, InvalidOperation, getcontext\n",
    "import logging\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from tqdm.notebook import tqdm # Ensure tqdm is imported for progress bar\n",
    "import uuid\n",
    "import os # To check log file path\n",
    "import matplotlib.pyplot as plt # Import Matplotlib here\n",
    "import matplotlib.ticker as mticker # For formatting y-axis\n",
    "\n",
    "# --- Prerequisites ---\\n\n",
    "# Ensure these are available from preceding cells\n",
    "if 'historical_data_test' not in locals() or historical_data_test.empty: raise RuntimeError(\"Run Cell 3 (Data Fetch) for historical_data_test\")\n",
    "if not all(v in globals() for v in ['price_tick_size_bt', 'min_qty_bt', 'qty_step_size_bt', 'min_notional_bt', 'adjust_price_bt', 'adjust_qty_bt', 'adjust_tp_price_bt']): raise RuntimeError(\"Run Cell 2 (Filters) for filters/helpers\")\n",
    "if 'rainbow_bands_df' not in locals() or rainbow_bands_df.empty: raise RuntimeError(\"Run Cell 5.1 (Rainbow Bands) for rainbow_bands_df.\")\n",
    "if 'get_band_index' not in globals(): raise RuntimeError(\"Run Cell 5.1 (Rainbow Bands) for get_band_index function.\")\n",
    "if 'df_lines' not in locals() or df_lines.empty: raise RuntimeError(\"Run Cell 5 (Power Law) for df_lines.\")\n",
    "\n",
    "\n",
    "# --- Configuration (v13.7.4) ---\\n\n",
    "getcontext().prec = 28\n",
    "# Logging Setup - Log DEBUG+ to file, INFO+ to screen (Ultra Quiet)\n",
    "LOG_FILENAME = 'backtest_log.txt'\n",
    "# Clear existing handlers\n",
    "for handler in logging.root.handlers[:]: logging.root.removeHandler(handler)\n",
    "# File Handler (DEBUG level)\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler = logging.FileHandler(LOG_FILENAME, mode='w') # Overwrite previous log\n",
    "file_handler.setFormatter(log_formatter)\n",
    "file_handler.setLevel(logging.DEBUG) # <<<< CAPTURE DEBUG TO FILE\n",
    "# Stream Handler (INFO level - for screen output)\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "stream_handler.setFormatter(log_formatter)\n",
    "stream_handler.setLevel(logging.INFO) # <<<< SHOW INFO ON SCREEN\n",
    "# Add handlers to the root logger\n",
    "logging.root.addHandler(file_handler)\n",
    "logging.root.addHandler(stream_handler)\n",
    "logging.root.setLevel(logging.DEBUG) # Root needs to be lowest level to capture all\n",
    "\n",
    "logger = logging.getLogger(__name__) # Get logger instance\n",
    "\n",
    "STARTING_QUOTE_BALANCE = Decimal('10000.0'); STARTING_BASE_BALANCE = Decimal('0.0') # Adjusted starting balance for demo\n",
    "SYMBOL = \"BTCUSDT\"\n",
    "QUOTE_ASSET = 'USDT'; BASE_ASSET = SYMBOL.replace(QUOTE_ASSET, '')\n",
    "FEE_RATE = Decimal('0.001');\n",
    "NUM_RAINBOW_BANDS = 100\n",
    "ALLOCATION_EXPONENT = Decimal('1.5')\n",
    "\n",
    "logging.info(f\"--- Starting Backtest Simulation v13.7.4 for {SYMBOL} (Quieter Fills) ---\") # v13.7.4\n",
    "logging.info(f\"Trigger: Price crossing ANY Rainbow Band boundary\")\n",
    "logging.info(f\"Context & Allocation: {NUM_RAINBOW_BANDS} Bands, Exp={ALLOCATION_EXPONENT}\")\n",
    "logging.info(f\"Rebalancing: Continuous adjustment on cross, trades executed if > MIN_NOTIONAL\")\n",
    "logging.info(f\"Logging DEBUG+ to: {os.path.abspath(LOG_FILENAME)}\") # Show log file path\n",
    "logging.info(f\"Simulated Starting Balance: {STARTING_QUOTE_BALANCE:,.2f} {QUOTE_ASSET}\")\n",
    "\n",
    "# --- Allocation Helper Function (Unchanged) ---\\n\n",
    "def target_crypto_allocation_pct(band_index: int, num_bands: int, exponent: Decimal) -> Decimal:\n",
    "    if band_index < 0 or band_index >= num_bands: return Decimal('0')\n",
    "    if num_bands <= 1: return Decimal('1.0')\n",
    "    linear_fraction = (Decimal(num_bands - 1 - band_index) / Decimal(num_bands - 1))\n",
    "    try:\n",
    "        target_pct = linear_fraction.copy_abs() ** exponent\n",
    "    except InvalidOperation:\n",
    "        logger.error(f\"Invalid operation during exponentiation: {linear_fraction} ** {exponent}\")\n",
    "        return Decimal('0')\n",
    "    if exponent % 1 != 0 and linear_fraction < 0:\n",
    "         logger.warning(f\"Attempting non-integer exponent on negative base ({linear_fraction}, {exponent}). Returning 0.\")\n",
    "         return Decimal('0')\n",
    "    return max(Decimal('0'), min(Decimal('1.0'), target_pct))\n",
    "\n",
    "# --- Simulation State Initialization (Unchanged) ---\\n\n",
    "quote_balance = STARTING_QUOTE_BALANCE; base_balance = STARTING_BASE_BALANCE\n",
    "trade_log = []; portfolio_history = []\n",
    "total_fees_paid = Decimal('0.0')\n",
    "\n",
    "# --- HODL Calculation Setup (Unchanged) ---\\n\n",
    "initial_hodl_price = historical_data_test['Close'].iloc[0]\n",
    "hodl_base_qty = (STARTING_QUOTE_BALANCE / initial_hodl_price) if initial_hodl_price > 0 else Decimal('0')\n",
    "hodl_portfolio_history = []\n",
    "logging.info(f\"Initial HODL Buy (adjusted): {hodl_base_qty:.8f} {BASE_ASSET} @ {initial_hodl_price:.2f}\")\n",
    "\n",
    "# --- Perform Initial Allocation (Unchanged) ---\\n\n",
    "logging.info(\"--- Performing Initial Allocation ---\")\n",
    "initial_buy_executed = False\n",
    "try:\n",
    "    first_timestamp = historical_data_test.index[0]\n",
    "    first_close = historical_data_test['Close'].iloc[0]\n",
    "    initial_band_index = get_band_index(first_close, first_timestamp, rainbow_bands_df, NUM_RAINBOW_BANDS)\n",
    "    if initial_band_index < 0:\n",
    "        logging.warning(f\"Initial price {first_close:.2f} outside bands. Starting 0% crypto.\")\n",
    "    else:\n",
    "        initial_target_pct = target_crypto_allocation_pct(initial_band_index, NUM_RAINBOW_BANDS, ALLOCATION_EXPONENT)\n",
    "        logger.info(f\"Initial Price: {first_close:.2f}, Band: {initial_band_index}, Target Alloc: {initial_target_pct:.2%}\") # Keep initial alloc INFO\n",
    "        initial_portfolio_value = STARTING_QUOTE_BALANCE; initial_target_crypto_value = initial_portfolio_value * initial_target_pct\n",
    "        initial_value_to_trade = initial_target_crypto_value\n",
    "        if initial_value_to_trade > 0:\n",
    "            adj_entry_price = adjust_price_bt(first_close, price_tick_size_bt)\n",
    "            if adj_entry_price and adj_entry_price > 0:\n",
    "                qty_to_buy_raw = initial_value_to_trade / adj_entry_price; qty_to_buy = adjust_qty_bt(qty_to_buy_raw, min_qty_bt, qty_step_size_bt)\n",
    "                buy_notional = adj_entry_price * qty_to_buy; cost_with_fee = buy_notional * (1 + FEE_RATE)\n",
    "                if buy_notional >= min_notional_bt and quote_balance >= cost_with_fee:\n",
    "                    quote_balance -= cost_with_fee; base_balance += qty_to_buy; total_fees_paid += buy_notional * FEE_RATE\n",
    "                    trade_log.append({'timestamp': first_timestamp, 'type': f\"INITIAL_ALLOC_BUY\", 'price': adj_entry_price, 'qty': qty_to_buy, 'value': -buy_notional, 'fee': buy_notional * FEE_RATE, 'signal_band': initial_band_index, 'target_pct': initial_target_pct})\n",
    "                    # *** Keep initial allocation confirmation as INFO for visibility on screen ***\n",
    "                    logging.info(f\"  +++ INITIAL ALLOCATION BUY Executed @ {adj_entry_price:.2f} Qty {qty_to_buy:.8f} +++\")\n",
    "                    logging.info(f\"  Initial Balance: Quote={quote_balance:,.2f}, Base={base_balance:.8f}\")\n",
    "                    initial_buy_executed = True\n",
    "                else: logging.warning(f\"  Skipped Initial Buy: Notional ({buy_notional:.4f} vs {min_notional_bt:.4f}) / Cost ({cost_with_fee:.4f} vs {quote_balance:.4f}) checks failed.\")\n",
    "            else: logging.warning(f\"  Skipped Initial Buy: Invalid adjusted entry price.\")\n",
    "except Exception as e_init:\n",
    "    logging.error(f\"Error during initial allocation: {e_init}\", exc_info=True)\n",
    "\n",
    "# --- Simulation Loop ---\\n\n",
    "logging.info(f\"Starting simulation loop v13.7.4 through {len(historical_data_test)} candles...\") # Log start\n",
    "candle_count = 0\n",
    "last_candle_close = historical_data_test['Close'].iloc[0] if not historical_data_test.empty else None\n",
    "\n",
    "for timestamp, candle_data in tqdm(historical_data_test.iterrows(), total=len(historical_data_test), desc=\"Backtest (v13.7.4 Quiet)\"):\n",
    "    candle_count += 1\n",
    "    current_high = candle_data['High']; current_low = candle_data['Low']; current_close = candle_data['Close']\n",
    "    timestamp_utc = timestamp\n",
    "\n",
    "    # --- A. Get Current Band Boundaries (Unchanged) ---\\n\n",
    "    try:\n",
    "        bands_row_index = rainbow_bands_df.index.asof(timestamp_utc)\n",
    "        if pd.isna(bands_row_index): raise ValueError(f\"Timestamp {timestamp_utc} not found in rainbow_bands_df using asof\")\n",
    "        band_boundaries = rainbow_bands_df.loc[bands_row_index]\n",
    "        if band_boundaries.isnull().any(): raise ValueError(\"NaNs in boundaries\")\n",
    "    except Exception as e_band:\n",
    "        logger.warning(f\"[{timestamp_utc}] Error getting band boundaries: {e_band}. Skipping rebalance check for this candle.\")\n",
    "        if last_candle_close is not None: last_candle_close = current_close\n",
    "        current_portfolio_value_eoc = quote_balance + (base_balance * current_close); current_hodl_value = hodl_base_qty * current_close\n",
    "        strategy_vs_hodl_pct = Decimal('NaN')\n",
    "        if current_hodl_value and current_hodl_value > 0:\n",
    "             try: strategy_vs_hodl_pct = (current_portfolio_value_eoc / current_hodl_value) * 100\n",
    "             except (InvalidOperation, ZeroDivisionError): strategy_vs_hodl_pct = Decimal('NaN')\n",
    "        portfolio_history.append({'timestamp': timestamp_utc, 'portfolio_value': current_portfolio_value_eoc, 'quote': quote_balance, 'base': base_balance, 'strategy_vs_hodl_pct': strategy_vs_hodl_pct})\n",
    "        hodl_portfolio_history.append({'timestamp': timestamp_utc, 'hodl_value': current_hodl_value})\n",
    "        continue\n",
    "\n",
    "    # --- B. Detect Band Crossing & Determine Target Band (Unchanged) ---\\n\n",
    "    target_band_index = -1; signal_price_for_calc = None; trade_direction = None\n",
    "    if last_candle_close is not None:\n",
    "        crossed_boundaries = []\n",
    "        for i in range(NUM_RAINBOW_BANDS + 1):\n",
    "            boundary_price = band_boundaries.get(f'Band_{i}', None)\n",
    "            if boundary_price is None or pd.isna(boundary_price): continue\n",
    "            boundary_price_dec = Decimal(str(boundary_price))\n",
    "            if min(current_low, last_candle_close) <= boundary_price_dec < max(current_high, last_candle_close):\n",
    "                 crossed_boundaries.append({'index': i, 'price': boundary_price_dec})\n",
    "        if crossed_boundaries:\n",
    "            if current_close < last_candle_close:\n",
    "                trade_direction = \"DOWN\"; crossed_boundaries.sort(key=lambda x: x['price'], reverse=True); highest_crossed_boundary = crossed_boundaries[0]\n",
    "                signal_price_for_calc = highest_crossed_boundary['price']; target_band_index = highest_crossed_boundary['index'] - 1\n",
    "                logger.debug(f\"[{timestamp_utc}] Crossed DOWN below Band_{highest_crossed_boundary['index']} @ ~{signal_price_for_calc:.2f} -> Target Band: {target_band_index}\")\n",
    "            elif current_close > last_candle_close:\n",
    "                trade_direction = \"UP\"; crossed_boundaries.sort(key=lambda x: x['price']); lowest_crossed_boundary = crossed_boundaries[0]\n",
    "                signal_price_for_calc = lowest_crossed_boundary['price']; target_band_index = lowest_crossed_boundary['index']\n",
    "                logger.debug(f\"[{timestamp_utc}] Crossed UP above Band_{target_band_index} @ ~{signal_price_for_calc:.2f} -> Target Band: {target_band_index}\")\n",
    "            else: # Flat close handling\n",
    "                 min_crossed = min(b['price'] for b in crossed_boundaries); max_crossed = max(b['price'] for b in crossed_boundaries)\n",
    "                 if current_close <= min_crossed:\n",
    "                     trade_direction = \"DOWN\"; crossed_boundaries.sort(key=lambda x: x['price'], reverse=True); highest_crossed_boundary = crossed_boundaries[0]\n",
    "                     signal_price_for_calc = highest_crossed_boundary['price']; target_band_index = highest_crossed_boundary['index'] - 1\n",
    "                     logger.debug(f\"[{timestamp_utc}] Flat Close, Crossed DOWN below Band_{highest_crossed_boundary['index']} @ ~{signal_price_for_calc:.2f} -> Target Band: {target_band_index}\")\n",
    "                 elif current_close >= max_crossed:\n",
    "                     trade_direction = \"UP\"; crossed_boundaries.sort(key=lambda x: x['price']); lowest_crossed_boundary = crossed_boundaries[0]\n",
    "                     signal_price_for_calc = lowest_crossed_boundary['price']; target_band_index = lowest_crossed_boundary['index']\n",
    "                     logger.debug(f\"[{timestamp_utc}] Flat Close, Crossed UP above Band_{target_band_index} @ ~{signal_price_for_calc:.2f} -> Target Band: {target_band_index}\")\n",
    "\n",
    "    # --- C. Execute Rebalancing Trade ---\\n\n",
    "    if target_band_index != -1 and signal_price_for_calc is not None:\n",
    "        target_band_index = max(0, min(NUM_RAINBOW_BANDS - 1, target_band_index))\n",
    "        logger.debug(f\"[{timestamp_utc}] Rebalance Triggered. Direction: {trade_direction}, Target Band: {target_band_index}, Signal Price: {signal_price_for_calc:.2f}\")\n",
    "        target_pct = target_crypto_allocation_pct(target_band_index, NUM_RAINBOW_BANDS, ALLOCATION_EXPONENT)\n",
    "        logger.debug(f\"  Target Crypto Allocation: {target_pct:.2%}\")\n",
    "\n",
    "        current_portfolio_value = quote_balance + (base_balance * signal_price_for_calc)\n",
    "        if current_portfolio_value <= 0:\n",
    "            logger.warning(f\"[{timestamp_utc}] Portfolio value is zero or negative ({current_portfolio_value:.2f}). Skipping rebalance.\")\n",
    "        else:\n",
    "            target_crypto_value = current_portfolio_value * target_pct; current_crypto_value = base_balance * signal_price_for_calc\n",
    "            value_to_trade = target_crypto_value - current_crypto_value; current_pct = current_crypto_value / current_portfolio_value\n",
    "            logger.debug(f\"  Current Value: {current_portfolio_value:,.2f} ({quote_balance:,.2f} Q + {current_crypto_value:,.2f} B | {current_pct:.2%})\")\n",
    "            logger.debug(f\"  Target Crypto Value: {target_crypto_value:,.2f} ({target_pct:.2%})\")\n",
    "            logger.debug(f\"  Value to Trade: {value_to_trade:,.2f}\")\n",
    "\n",
    "            trade_executed = False; min_trade_value_threshold = min_notional_bt * Decimal(\"0.5\")\n",
    "            if abs(value_to_trade) > min_trade_value_threshold:\n",
    "                if value_to_trade > 0: # BUY Rebalance\n",
    "                    adj_entry_price = adjust_price_bt(signal_price_for_calc, price_tick_size_bt)\n",
    "                    if not adj_entry_price or adj_entry_price <= 0: logger.warning(f\"  Adjusted BUY price invalid ({adj_entry_price}).\")\n",
    "                    else:\n",
    "                        quote_balance = Decimal(str(quote_balance)); qty_to_buy_raw = value_to_trade / adj_entry_price; qty_to_buy = adjust_qty_bt(qty_to_buy_raw, min_qty_bt, qty_step_size_bt)\n",
    "                        buy_notional = adj_entry_price * qty_to_buy; cost_with_fee = buy_notional * (1 + FEE_RATE)\n",
    "                        logger.debug(f\"  BUY Needed: Adj Price={adj_entry_price:.4f}, Adj Qty={qty_to_buy:.8f}, Notional={buy_notional:.4f}, Cost={cost_with_fee:.4f}, Avail Quote={quote_balance:.4f}\")\n",
    "                        if buy_notional >= min_notional_bt and quote_balance >= cost_with_fee:\n",
    "                            quote_balance -= cost_with_fee; base_balance += qty_to_buy; total_fees_paid += buy_notional * FEE_RATE\n",
    "                            trade_log.append({'timestamp': timestamp_utc, 'type': f\"REBALANCE_BUY\", 'price': adj_entry_price, 'qty': qty_to_buy, 'value': -buy_notional, 'fee': buy_notional * FEE_RATE, 'target_band': target_band_index, 'target_pct': target_pct, 'current_pct': current_pct})\n",
    "                            # *** CHANGED to DEBUG ***\n",
    "                            logger.debug(f\"    +++ REBALANCE BUY Executed @ {adj_entry_price:.2f} Qty {qty_to_buy:.8f} +++\"); trade_executed = True\n",
    "                        else: logger.debug(f\"    Skipped BUY: Filter fail (Notional {buy_notional:.4f} vs Min {min_notional_bt:.4f} OR Quote {quote_balance:.4f} vs Cost {cost_with_fee:.4f})\")\n",
    "                elif value_to_trade < 0: # SELL Rebalance\n",
    "                    adj_exit_price = adjust_tp_price_bt(signal_price_for_calc, price_tick_size_bt)\n",
    "                    if not adj_exit_price or adj_exit_price <= 0: logger.warning(f\"  Adjusted SELL price invalid ({adj_exit_price}).\")\n",
    "                    else:\n",
    "                        base_balance = Decimal(str(base_balance)); qty_to_sell_raw = abs(value_to_trade) / adj_exit_price; qty_to_sell = adjust_qty_bt(qty_to_sell_raw, min_qty_bt, qty_step_size_bt)\n",
    "                        qty_to_sell = min(qty_to_sell, base_balance); sell_notional = adj_exit_price * qty_to_sell\n",
    "                        logger.debug(f\"  SELL Needed: Adj Price={adj_exit_price:.4f}, Adj Qty={qty_to_sell:.8f}, Notional={sell_notional:.4f}, Avail Base={base_balance:.8f}\")\n",
    "                        if sell_notional >= min_notional_bt and qty_to_sell > 0:\n",
    "                            gross_proceeds = sell_notional; fee = gross_proceeds * FEE_RATE; net_proceeds = gross_proceeds - fee\n",
    "                            quote_balance += net_proceeds; base_balance -= qty_to_sell; total_fees_paid += fee\n",
    "                            trade_log.append({'timestamp': timestamp_utc, 'type': f\"REBALANCE_SELL\", 'price': adj_exit_price, 'qty': qty_to_sell, 'value': net_proceeds, 'fee': fee, 'target_band': target_band_index, 'target_pct': target_pct, 'current_pct': current_pct})\n",
    "                            # *** CHANGED to DEBUG ***\n",
    "                            logger.debug(f\"    +++ REBALANCE SELL Executed @ {adj_exit_price:.2f} Qty {qty_to_sell:.8f} +++\"); trade_executed = True\n",
    "                        else: logger.debug(f\"    Skipped SELL: Filter fail (Notional {sell_notional:.4f} vs Min {min_notional_bt:.4f} OR Qty <= 0)\")\n",
    "            else: logger.debug(f\"  No trade needed (abs(value_to_trade) {abs(value_to_trade):.4f} <= threshold {min_trade_value_threshold:.4f}).\")\n",
    "            if trade_executed:\n",
    "                 logger.debug(f\"  New Balance: Quote={quote_balance:,.2f}, Base={base_balance:.8f}\")\n",
    "                 post_trade_port_value = quote_balance + (base_balance * signal_price_for_calc);\n",
    "                 if post_trade_port_value > 0: logger.debug(f\"  Post-Trade Crypto Alloc: {(base_balance * signal_price_for_calc)/post_trade_port_value:.2%} (Target was: {target_pct:.2%})\")\n",
    "\n",
    "    # --- D. Record Portfolio Values & Update Last Close (Unchanged) ---\\n\n",
    "    current_portfolio_value_eoc = quote_balance + (base_balance * current_close); current_hodl_value = hodl_base_qty * current_close\n",
    "    strategy_vs_hodl_pct = Decimal('NaN')\n",
    "    if current_hodl_value and current_hodl_value > 0:\n",
    "        try: strategy_vs_hodl_pct = (current_portfolio_value_eoc / current_hodl_value) * 100\n",
    "        except (InvalidOperation, ZeroDivisionError): strategy_vs_hodl_pct = Decimal('NaN')\n",
    "    portfolio_history.append({'timestamp': timestamp_utc, 'portfolio_value': current_portfolio_value_eoc, 'quote': quote_balance, 'base': base_balance, 'strategy_vs_hodl_pct': strategy_vs_hodl_pct})\n",
    "    hodl_portfolio_history.append({'timestamp': timestamp_utc, 'hodl_value': current_hodl_value})\n",
    "    last_candle_close = current_close\n",
    "\n",
    "# --- Loop End ---\\n\n",
    "logging.info(f\"--- Simulation Loop Finished ---\\n\") # Log finish\n",
    "\n",
    "# --- Final Portfolio Calculation & Performance Metrics (Unchanged) ---\\n\n",
    "trades_df = pd.DataFrame(trade_log)\n",
    "if not trades_df.empty:\n",
    "     trades_df['timestamp'] = pd.to_datetime(trades_df['timestamp'])\n",
    "     for col in ['price', 'qty', 'value', 'fee', 'target_pct', 'current_pct']:\n",
    "         if col in trades_df.columns: trades_df[col] = trades_df[col].apply(lambda x: Decimal(str(x)) if pd.notna(x) and x is not None else Decimal('NaN'))\n",
    "\n",
    "final_quote_balance = quote_balance; final_base_balance = base_balance\n",
    "last_close_price = historical_data_test['Close'].iloc[-1] if not historical_data_test.empty else Decimal('0')\n",
    "final_portfolio_value = final_quote_balance + (final_base_balance * last_close_price)\n",
    "final_hodl_value = hodl_base_qty * last_close_price\n",
    "print(\"\\n--- Backtest Results (v13.7.4 - Quieter Fills) ---\") # Note version\n",
    "print(f\"Simulation Period: {historical_data_test.index.min()} to {historical_data_test.index.max()}\")\n",
    "print(f\"Trigger: Price crossing ANY Rainbow Band boundary\"); print(f\"Context & Allocation: {NUM_RAINBOW_BANDS} Bands, Exponent={ALLOCATION_EXPONENT}\")\n",
    "print(f\"Rebalancing: Continuous adjustment on cross, Skipped if < MIN_NOTIONAL\"); print(f\"Fee Rate Applied: {FEE_RATE*100}%\\\\n\")\n",
    "print(f\"Initial Portfolio Value: {STARTING_QUOTE_BALANCE:,.2f} {QUOTE_ASSET}\"); print(f\"Final Portfolio Value (Strategy): {final_portfolio_value:,.2f} {QUOTE_ASSET}\")\n",
    "total_profit_loss = final_portfolio_value - STARTING_QUOTE_BALANCE; total_profit_loss_percent = (total_profit_loss / STARTING_QUOTE_BALANCE) * 100 if STARTING_QUOTE_BALANCE > 0 else Decimal('0')\n",
    "print(f\"Total Profit/Loss (Strategy): {total_profit_loss:,.2f} {QUOTE_ASSET} ({total_profit_loss_percent:.2f}%)\\\"\")\n",
    "print(f\"Final Portfolio Value (HODL): {final_hodl_value:,.2f} {QUOTE_ASSET}\")\n",
    "hodl_profit_loss = final_hodl_value - STARTING_QUOTE_BALANCE; hodl_profit_loss_percent = (hodl_profit_loss / STARTING_QUOTE_BALANCE) * 100 if STARTING_QUOTE_BALANCE > 0 else Decimal('0')\n",
    "print(f\"Total Profit/Loss (HODL): {hodl_profit_loss:,.2f} {QUOTE_ASSET} ({hodl_profit_loss_percent:.2f}%)\\\"\")\n",
    "\n",
    "perf_vs_hodl_pct = Decimal('NaN');\n",
    "if final_hodl_value and final_hodl_value > 0: perf_vs_hodl_pct = (final_portfolio_value / final_hodl_value) * 100\n",
    "print(f\"Strategy Performance vs HODL: {perf_vs_hodl_pct:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal Rebalancing Trades Executed: {len(trades_df)}\")\n",
    "if not trades_df.empty:\n",
    "    initial_trades = trades_df[trades_df['type'] == 'INITIAL_ALLOC_BUY']; buy_trades = trades_df[trades_df['type'] == 'REBALANCE_BUY']; sell_trades = trades_df[trades_df['type'] == 'REBALANCE_SELL']\n",
    "    print(f\"  Initial Alloc Trades: {len(initial_trades)}\"); print(f\"  Buy Rebalances: {len(buy_trades)}\"); print(f\"  Sell Rebalances: {len(sell_trades)}\")\n",
    "    total_buy_value = abs(buy_trades['value'].dropna().sum()) if not buy_trades.empty else Decimal('0'); total_initial_value = abs(initial_trades['value'].dropna().sum()) if not initial_trades.empty else Decimal('0'); total_sell_value = sell_trades['value'].dropna().sum() if not sell_trades.empty else Decimal('0')\n",
    "    print(f\"  Total Buy Value (Quote): {(total_buy_value + total_initial_value):,.2f}\"); print(f\"  Total Sell Value (Quote): {total_sell_value:,.2f}\")\n",
    "else: print(\"  Buy Rebalances: 0\\\\n  Sell Rebalances: 0\");\n",
    "print(f\"Total Fees Paid (Strategy): {total_fees_paid:,.4f} {QUOTE_ASSET}\")\n",
    "\n",
    "# --- Plotting (Axes Flipped, Price Scaled) --- V4 (Unchanged Plotting) ---\n",
    "try:\n",
    "    portfolio_df = pd.DataFrame(portfolio_history).set_index('timestamp')\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8)) # ax1 is now Price axis (Left)\n",
    "\n",
    "    # --- Plot 1: Price + PL Lines + Trades (LEFT AXIS) ---\n",
    "    color = 'black'; ax1.set_xlabel('Date'); ax1.set_ylabel('BTC Price (USDT)', color=color)\n",
    "    price_line, = ax1.plot(historical_data_test.index, historical_data_test['Close'].astype(float), color='grey', linewidth=0.75, alpha=0.8, label='BTC Price (Close)')\n",
    "    ax1.tick_params(axis='y', labelcolor=color); price_formatter = mticker.FormatStrFormatter('%.0f'); ax1.yaxis.set_major_formatter(price_formatter)\n",
    "    ax1.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "    pl_s_line = None; pl_r_line = None\n",
    "    if not df_lines.empty:\n",
    "        df_lines_aligned = df_lines.reindex(historical_data_test.index, method='ffill')\n",
    "        pl_s_line, = ax1.plot(df_lines_aligned.index, df_lines_aligned['Support'].astype(float), color='red', linestyle='--', linewidth=1, alpha=0.7, label='PL Support')\n",
    "        pl_r_line, = ax1.plot(df_lines_aligned.index, df_lines_aligned['Resistance'].astype(float), color='purple', linestyle='--', linewidth=1, alpha=0.7, label='PL Resistance')\n",
    "\n",
    "    buy_plot = None; sell_plot = None\n",
    "    if not trades_df.empty:\n",
    "        trades_df['price_float'] = trades_df['price'].apply(lambda x: float(x) if isinstance(x, Decimal) and not x.is_nan() else np.nan)\n",
    "        buy_markers = trades_df[(trades_df['type'] == 'REBALANCE_BUY') | (trades_df['type'] == 'INITIAL_ALLOC_BUY')].dropna(subset=['price_float'])\n",
    "        sell_markers = trades_df[trades_df['type'] == 'REBALANCE_SELL'].dropna(subset=['price_float'])\n",
    "        if not buy_markers.empty: buy_plot = ax1.scatter(buy_markers['timestamp'], buy_markers['price_float'] * 1.001, label='Buy Executed', marker='^', color='lime', s=50, alpha=0.9, zorder=5)\n",
    "        if not sell_markers.empty: sell_plot = ax1.scatter(sell_markers['timestamp'], sell_markers['price_float'] * 0.999, label='Sell Executed', marker='v', color='red', s=50, alpha=0.9, zorder=5)\n",
    "\n",
    "    # --- Plot 2: Strategy Performance vs HODL (%) (RIGHT AXIS) ---\n",
    "    ax2 = ax1.twinx(); color = 'tab:blue'; ax2.set_ylabel('Strategy Value (% of HODL)', color=color)\n",
    "    portfolio_df['strategy_vs_hodl_pct_float'] = portfolio_df['strategy_vs_hodl_pct'].apply(lambda x: float(x) if isinstance(x, Decimal) and not x.is_nan() else np.nan)\n",
    "    strat_pct_line, = ax2.plot(portfolio_df.index, portfolio_df['strategy_vs_hodl_pct_float'], color=color, linewidth=1.5, label=f'Strategy Perf vs HODL (%)')\n",
    "    hodl_baseline = ax2.axhline(100, color=color, linestyle=':', linewidth=1, label='100% (HODL Baseline)', alpha=0.7)\n",
    "    ax2.tick_params(axis='y', labelcolor=color); ax2.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=100)); ax2.grid(False)\n",
    "\n",
    "    # --- Final Formatting ---\n",
    "    handles_ax1, labels_ax1 = [price_line], [price_line.get_label()]\n",
    "    if pl_s_line: handles_ax1.append(pl_s_line); labels_ax1.append(pl_s_line.get_label())\n",
    "    if pl_r_line: handles_ax1.append(pl_r_line); labels_ax1.append(pl_r_line.get_label())\n",
    "    if buy_plot: handles_ax1.append(buy_plot); labels_ax1.append(buy_plot.get_label())\n",
    "    if sell_plot: handles_ax1.append(sell_plot); labels_ax1.append(sell_plot.get_label())\n",
    "    handles_ax2, labels_ax2 = [strat_pct_line, hodl_baseline], [strat_pct_line.get_label(), hodl_baseline.get_label()]\n",
    "    ax1.legend(handles_ax1 + handles_ax2, labels_ax1 + labels_ax2, loc='upper left')\n",
    "\n",
    "    # Adjust Y-axis limits\n",
    "    min_price_val = historical_data_test['Low'].astype(float).min(); max_price_val = historical_data_test['High'].astype(float).max()\n",
    "    if pd.notna(min_price_val) and pd.notna(max_price_val): ax1.set_ylim(bottom=float(min_price_val) * 0.99, top=float(max_price_val) * 1.01)\n",
    "    else: logger.warning(\"Could not determine valid price range for Y-axis limit on ax1.\")\n",
    "    min_pct = portfolio_df['strategy_vs_hodl_pct_float'].dropna().min(); max_pct = portfolio_df['strategy_vs_hodl_pct_float'].dropna().max()\n",
    "    lower_bound = min(min_pct * 0.98, 90) if pd.notna(min_pct) else 90; upper_bound = max(max_pct * 1.02, 110) if pd.notna(max_pct) else 110\n",
    "    ax2.set_ylim(bottom=lower_bound, top=upper_bound)\n",
    "\n",
    "    freq_str = getattr(historical_data_test.index, 'freqstr', None) or \"Hourly\"\n",
    "    fig.suptitle(f'Backtest v13.7.4: Price Context (L) & Strategy Perf vs HODL % (R) - {SYMBOL} ({freq_str})')\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.show()\n",
    "\n",
    "except ImportError: logging.warning(\"matplotlib not installed. Skipping plot generation. `pip install matplotlib`\")\n",
    "except Exception as plot_err: logging.error(f\"Error during plotting: {plot_err}\", exc_info=True)\n",
    "\n",
    "\n",
    "# End of Cell 6 (v13.7.4 - Quieter Fills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718936d-1267-4499-9f7d-e6137474977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Plot Backtest Results with Rainbow Band Shading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.colors as mcolors # For colormaps\n",
    "import logging\n",
    "from decimal import Decimal # Although not used for calcs here, good practice\n",
    "\n",
    "# --- Prerequisites ---\n",
    "logger = logging.getLogger(__name__) # Get logger instance\n",
    "\n",
    "# Check if necessary DataFrames and variables exist from previous cells\n",
    "required_vars = ['historical_data_test', 'portfolio_df', 'trades_df', 'df_lines', 'rainbow_bands_df', 'NUM_RAINBOW_BANDS', 'SYMBOL']\n",
    "for var in required_vars:\n",
    "    if var not in locals() and var not in globals():\n",
    "         raise NameError(f\"Variable '{var}' not found. Please run previous cells (especially Cell 6) first.\")\n",
    "\n",
    "if portfolio_df.empty or historical_data_test.empty:\n",
    "     raise ValueError(\"Portfolio history or historical test data is empty. Run Cell 6.\")\n",
    "\n",
    "\n",
    "# --- Plotting (Rainbow Bands Shaded) --- V5 ---\n",
    "logging.info(\"--- Generating Plot with Rainbow Band Shading ---\")\n",
    "try:\n",
    "    # Make copies to avoid modifying original dfs if needed elsewhere\n",
    "    portfolio_df_plot = portfolio_df.copy()\n",
    "    trades_df_plot = trades_df.copy() if not trades_df.empty else pd.DataFrame() # Handle empty trades_df\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8)) # ax1 is Price axis (Left)\n",
    "\n",
    "    # --- Plot 1: Price + PL Lines + Trades + RAINBOW BANDS (LEFT AXIS) ---\n",
    "    color = 'black'; ax1.set_xlabel('Date'); ax1.set_ylabel('BTC Price (USDT)', color=color)\n",
    "    # Plot price first\n",
    "    price_line, = ax1.plot(historical_data_test.index, historical_data_test['Close'].astype(float), color='dimgrey', linewidth=1.0, alpha=0.9, label='BTC Price (Close)', zorder=4) # Increase zorder\n",
    "    ax1.tick_params(axis='y', labelcolor=color); price_formatter = mticker.FormatStrFormatter('%.0f'); ax1.yaxis.set_major_formatter(price_formatter)\n",
    "    ax1.grid(True, linestyle=':', alpha=0.4, zorder=0) # Send grid behind everything\n",
    "\n",
    "    # Align daily data to hourly index for plotting\n",
    "    plot_index = historical_data_test.index # Use the index of the main backtest data\n",
    "    if not df_lines.empty:\n",
    "        df_lines_aligned = df_lines.reindex(plot_index, method='ffill')\n",
    "    if not rainbow_bands_df.empty:\n",
    "        rainbow_bands_aligned = rainbow_bands_df.reindex(plot_index, method='ffill')\n",
    "\n",
    "    # --- Plot Rainbow Bands Shading ---\n",
    "    if not rainbow_bands_aligned.empty:\n",
    "        # Choose a colormap (e.g., 'rainbow', 'viridis', 'coolwarm', 'RdYlGn')\n",
    "        cmap = plt.cm.rainbow # Or plt.cm.viridis, plt.cm.coolwarm etc.\n",
    "        band_alpha = 0.10 # Transparency of the bands\n",
    "\n",
    "        # Ensure band columns are float for plotting\n",
    "        for i in range(NUM_RAINBOW_BANDS + 1):\n",
    "             col_name = f'Band_{i}'\n",
    "             if col_name in rainbow_bands_aligned.columns:\n",
    "                  rainbow_bands_aligned[col_name] = rainbow_bands_aligned[col_name].astype(float)\n",
    "\n",
    "\n",
    "        # Loop through bands and fill_between\n",
    "        for i in range(NUM_RAINBOW_BANDS):\n",
    "            lower_band_col = f'Band_{i}'\n",
    "            upper_band_col = f'Band_{i+1}'\n",
    "            # Check if columns exist\n",
    "            if lower_band_col in rainbow_bands_aligned.columns and upper_band_col in rainbow_bands_aligned.columns:\n",
    "                color_val = cmap(i / NUM_RAINBOW_BANDS) # Normalize index to 0-1 for colormap\n",
    "                ax1.fill_between(rainbow_bands_aligned.index,\n",
    "                                 rainbow_bands_aligned[lower_band_col],\n",
    "                                 rainbow_bands_aligned[upper_band_col],\n",
    "                                 color=color_val,\n",
    "                                 alpha=band_alpha,\n",
    "                                 linewidth=0, # No lines between bands\n",
    "                                 zorder=1) # Place bands behind price/trades\n",
    "            # else:\n",
    "                # logger.warning(f\"Columns missing for band {i}: {lower_band_col}, {upper_band_col}\")\n",
    "\n",
    "    # Plot Power Law Lines on ax1 (after bands)\n",
    "    pl_s_line = None; pl_r_line = None\n",
    "    if not df_lines_aligned.empty:\n",
    "        pl_s_line, = ax1.plot(df_lines_aligned.index, df_lines_aligned['Support'].astype(float), color='maroon', linestyle='--', linewidth=1.5, alpha=0.8, label='PL Support', zorder=3) # Darker red\n",
    "        pl_r_line, = ax1.plot(df_lines_aligned.index, df_lines_aligned['Resistance'].astype(float), color='indigo', linestyle='--', linewidth=1.5, alpha=0.8, label='PL Resistance', zorder=3) # Darker purple\n",
    "\n",
    "    # Plot Trade Markers on ax1 (after bands)\n",
    "    buy_plot = None; sell_plot = None\n",
    "    if not trades_df_plot.empty:\n",
    "        # Ensure price_float column exists and handle potential prior creation\n",
    "        if 'price_float' not in trades_df_plot.columns:\n",
    "             trades_df_plot['price_float'] = trades_df_plot['price'].apply(lambda x: float(x) if isinstance(x, Decimal) and not x.is_nan() else np.nan)\n",
    "\n",
    "        buy_markers = trades_df_plot[(trades_df_plot['type'] == 'REBALANCE_BUY') | (trades_df_plot['type'] == 'INITIAL_ALLOC_BUY')].dropna(subset=['price_float'])\n",
    "        sell_markers = trades_df_plot[trades_df_plot['type'] == 'REBALANCE_SELL'].dropna(subset=['price_float'])\n",
    "        if not buy_markers.empty: buy_plot = ax1.scatter(buy_markers['timestamp'], buy_markers['price_float'] * 1.001, label='Buy Executed', marker='^', color='lime', edgecolor='black', linewidth=0.5, s=60, alpha=1.0, zorder=5) # Adjusted appearance\n",
    "        if not sell_markers.empty: sell_plot = ax1.scatter(sell_markers['timestamp'], sell_markers['price_float'] * 0.999, label='Sell Executed', marker='v', color='red', edgecolor='black', linewidth=0.5, s=60, alpha=1.0, zorder=5) # Adjusted appearance\n",
    "\n",
    "\n",
    "    # --- Plot 2: Strategy Performance vs HODL (%) (RIGHT AXIS) ---\n",
    "    ax2 = ax1.twinx(); color = 'tab:blue'; ax2.set_ylabel('Strategy Value (% of HODL)', color=color)\n",
    "    if 'strategy_vs_hodl_pct_float' not in portfolio_df_plot.columns: # Calculate if not done in cell 6 history recording\n",
    "        portfolio_df_plot['strategy_vs_hodl_pct_float'] = portfolio_df_plot['strategy_vs_hodl_pct'].apply(lambda x: float(x) if isinstance(x, Decimal) and not x.is_nan() else np.nan)\n",
    "\n",
    "    strat_pct_line, = ax2.plot(portfolio_df_plot.index, portfolio_df_plot['strategy_vs_hodl_pct_float'], color=color, linewidth=1.5, label=f'Strategy Perf vs HODL (%)')\n",
    "    hodl_baseline = ax2.axhline(100, color=color, linestyle=':', linewidth=1, label='100% (HODL Baseline)', alpha=0.7)\n",
    "    ax2.tick_params(axis='y', labelcolor=color); ax2.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=100)); ax2.grid(False)\n",
    "\n",
    "    # --- Final Formatting ---\n",
    "    # Collect handles manually\n",
    "    handles_ax1, labels_ax1 = [price_line], [price_line.get_label()]\n",
    "    if pl_s_line: handles_ax1.append(pl_s_line); labels_ax1.append(pl_s_line.get_label())\n",
    "    if pl_r_line: handles_ax1.append(pl_r_line); labels_ax1.append(pl_r_line.get_label())\n",
    "    if buy_plot: handles_ax1.append(buy_plot); labels_ax1.append(buy_plot.get_label())\n",
    "    if sell_plot: handles_ax1.append(sell_plot); labels_ax1.append(sell_plot.get_label())\n",
    "    handles_ax2, labels_ax2 = [strat_pct_line, hodl_baseline], [strat_pct_line.get_label(), hodl_baseline.get_label()]\n",
    "    # Place combined legend on ax1\n",
    "    ax1.legend(handles_ax1 + handles_ax2, labels_ax1 + labels_ax2, loc='upper left', fontsize='small') # Adjust font size if needed\n",
    "\n",
    "    # Adjust Y-axis limits\n",
    "    min_price_val = historical_data_test['Low'].astype(float).min(); max_price_val = historical_data_test['High'].astype(float).max()\n",
    "    if pd.notna(min_price_val) and pd.notna(max_price_val): ax1.set_ylim(bottom=float(min_price_val) * 0.99, top=float(max_price_val) * 1.01)\n",
    "    else: logger.warning(\"Could not determine valid price range for Y-axis limit on ax1.\")\n",
    "\n",
    "    # Calculate % axis limits based on plotted data\n",
    "    min_pct_val = portfolio_df_plot['strategy_vs_hodl_pct_float'].dropna().min()\n",
    "    max_pct_val = portfolio_df_plot['strategy_vs_hodl_pct_float'].dropna().max()\n",
    "    lower_pct_bound = min(min_pct_val * 0.98, 90) if pd.notna(min_pct_val) else 85 # Adjust default lower if needed\n",
    "    upper_pct_bound = max(max_pct_val * 1.02, 110) if pd.notna(max_pct_val) else 115 # Adjust default upper if needed\n",
    "    ax2.set_ylim(bottom=lower_pct_bound, top=upper_pct_bound)\n",
    "\n",
    "\n",
    "    # --- Title ---\n",
    "    # Attempt to get version from Cell 6 output, otherwise use placeholder\n",
    "    backtest_version = \"v13.7.x\" # Placeholder\n",
    "    # This part is tricky as we don't have direct access to Cell 6's print output here.\n",
    "    # We'll stick to a generic title or manually update if needed.\n",
    "    # Could potentially parse the log file if absolutely necessary, but likely overkill.\n",
    "    fig.suptitle(f'Backtest Analysis: Price Context (L) & Strategy Perf vs HODL % (R) - {SYMBOL} (Rainbow Bands)', fontsize=14)\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.show()\n",
    "\n",
    "except NameError as ne:\n",
    "     logger.error(f\"Plotting Error: {ne}. Make sure prerequisite cells (esp. Cell 6) have been run.\")\n",
    "except ValueError as ve:\n",
    "     logger.error(f\"Plotting Error: {ve}. Check if DataFrames are empty.\")\n",
    "except Exception as plot_err:\n",
    "    logger.error(f\"Error during plotting: {plot_err}\", exc_info=True)\n",
    "\n",
    "\n",
    "# End of Cell 7: Plot Backtest Results with Rainbow Band Shading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd2724-b9fe-426c-a1bf-49812966f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 (v3.0 - Verify Input Data & Set Price Y-Limit FIRST): Plot Backtest Results\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import logging\n",
    "from decimal import Decimal # Although not used for calcs here, good practice\n",
    "\n",
    "# --- Prerequisites ---\n",
    "logger = logging.getLogger(__name__) # Get logger instance\n",
    "\n",
    "# Check if necessary DataFrames and variables exist from previous cells\n",
    "required_vars = ['historical_data_test', 'portfolio_df', 'trades_df', 'df_lines', 'rainbow_bands_df', 'NUM_RAINBOW_BANDS', 'SYMBOL']\n",
    "for var in required_vars:\n",
    "    if var not in locals() and var not in globals():\n",
    "         raise NameError(f\"Variable '{var}' not found. Please run previous cells (especially Cell 6) first.\")\n",
    "\n",
    "# *** ADDED VERIFICATION STEP ***\n",
    "logger.info(\"--- Verifying input historical_data_test before plotting ---\")\n",
    "if 'historical_data_test' in locals() or 'historical_data_test' in globals():\n",
    "    if not historical_data_test.empty:\n",
    "        try:\n",
    "            max_high_in_input = historical_data_test['High'].astype(float).max()\n",
    "            min_low_in_input = historical_data_test['Low'].astype(float).min()\n",
    "            logger.info(f\"VERIFICATION: historical_data_test['High'].max() = {max_high_in_input}\")\n",
    "            logger.info(f\"VERIFICATION: historical_data_test['Low'].min() = {min_low_in_input}\")\n",
    "            expected_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'ATR_'] # Check for expected cols prefix\n",
    "            actual_cols = historical_data_test.columns.tolist()\n",
    "            logger.info(f\"VERIFICATION: historical_data_test columns: {actual_cols}\")\n",
    "            if max_high_in_input > 50000: # Threshold suggesting contamination\n",
    "                 logger.warning(\"!!! VERIFICATION FAILED: Max high in historical_data_test seems too large (> 50k), likely contaminated!\")\n",
    "            else:\n",
    "                 logger.info(\"VERIFICATION: Max high seems reasonable (< 50k).\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during verification: {e}\")\n",
    "    else:\n",
    "        logger.error(\"VERIFICATION: historical_data_test is empty!\")\n",
    "else:\n",
    "    logger.error(\"VERIFICATION: historical_data_test not found!\")\n",
    "# ******************************\n",
    "\n",
    "# Check required DFs (redundant but safe)\n",
    "if 'historical_data_test' not in globals() or historical_data_test.empty:\n",
    "     raise ValueError(\"historical_data_test is empty. Run Cell 3/6.\")\n",
    "if 'rainbow_bands_df' not in globals() or rainbow_bands_df.empty:\n",
    "      raise ValueError(\"rainbow_bands_df is empty. Run Cell 5.1.\")\n",
    "if 'trades_df' not in globals():\n",
    "     raise ValueError(\"trades_df is missing. Run Cell 6.\")\n",
    "\n",
    "\n",
    "# --- Plotting (Set Price Y-Limit FIRST) --- V16 ---\n",
    "logging.info(\"--- Generating Plot: Price Focus (Setting Y-Limit FIRST) ---\")\n",
    "try:\n",
    "    trades_df_plot = trades_df.copy() if not trades_df.empty else pd.DataFrame()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 8)) # Only one axis needed: ax1 is Price axis (Left)\n",
    "\n",
    "    # --- *** STEP 1: Calculate and SET Y-Limits for Price Axis FIRST *** ---\n",
    "    # Use the VERIFIED input max/min if possible\n",
    "    min_price_val = historical_data_test['Low'].astype(float).min()\n",
    "    max_price_val = historical_data_test['High'].astype(float).max()\n",
    "\n",
    "    # *** Check if max_price_val is still contaminated despite verification ***\n",
    "    # If it seems contaminated, override it for plotting purposes\n",
    "    if max_price_val > 50000: # Use 50k as a sanity check threshold\n",
    "        logger.warning(f\"Overriding calculated max_price_val ({max_price_val}) with 50000 for plot scaling due to suspected contamination.\")\n",
    "        max_price_val_plot = 50000.0\n",
    "    else:\n",
    "        max_price_val_plot = max_price_val # Use the calculated value if it seems okay\n",
    "\n",
    "    if pd.notna(min_price_val) and pd.notna(max_price_val_plot):\n",
    "         padding_pct = 0.015\n",
    "         price_range = max_price_val_plot - min_price_val\n",
    "         if price_range <= 0: price_range = max_price_val_plot * 0.01 if max_price_val_plot > 0 else 1.0\n",
    "         plot_bottom = float(min_price_val) - (price_range * padding_pct)\n",
    "         plot_top = float(max_price_val_plot) + (price_range * padding_pct)\n",
    "\n",
    "         ax1.set_ylim(bottom=plot_bottom, top=plot_top)\n",
    "         logger.info(f\"PRE-SETTING Y-limit for Price axis (ax1) based on data range [{min_price_val:.2f}, {max_price_val_plot:.2f}] -> [{plot_bottom:.2f}, {plot_top:.2f}]\")\n",
    "    else:\n",
    "        logger.warning(\"Could not determine valid price range; using default Y-limit for ax1.\")\n",
    "\n",
    "    # --- *** STEP 2: Now Plot Everything onto the Pre-Scaled Axis *** ---\n",
    "    color = 'black'; ax1.set_xlabel('Date'); ax1.set_ylabel('BTC Price (USDT)', color=color)\n",
    "    price_line, = ax1.plot(historical_data_test.index, historical_data_test['Close'].astype(float), color='dimgrey', linewidth=1.0, alpha=0.9, label='BTC Price (Close)', zorder=4)\n",
    "    ax1.tick_params(axis='y', labelcolor=color); price_formatter = mticker.FormatStrFormatter('%.0f'); ax1.yaxis.set_major_formatter(price_formatter)\n",
    "    ax1.grid(True, linestyle=':', alpha=0.4, zorder=0)\n",
    "\n",
    "    # Align daily data\n",
    "    plot_index = historical_data_test.index\n",
    "    df_lines_aligned = pd.DataFrame(); rainbow_bands_aligned = pd.DataFrame()\n",
    "    if not df_lines.empty: df_lines_aligned = df_lines.reindex(plot_index, method='ffill')\n",
    "    if not rainbow_bands_df.empty: rainbow_bands_aligned = rainbow_bands_df.reindex(plot_index, method='ffill')\n",
    "\n",
    "    # Plot Rainbow Band Boundary Lines\n",
    "    band_line_color = 'lightgrey'; band_line_style = ':'; band_line_width = 0.5; band_zorder = 1\n",
    "    plotted_band_line = False\n",
    "    if not rainbow_bands_aligned.empty:\n",
    "        for i in range(1, NUM_RAINBOW_BANDS):\n",
    "             col_name = f'Band_{i}'\n",
    "             if col_name in rainbow_bands_aligned.columns:\n",
    "                  band_data_float = rainbow_bands_aligned[col_name].astype(float)\n",
    "                  ax1.plot(rainbow_bands_aligned.index, band_data_float, color=band_line_color, linestyle=band_line_style, linewidth=band_line_width, zorder=band_zorder, label='Rainbow Boundaries' if not plotted_band_line else \"\")\n",
    "                  plotted_band_line = True\n",
    "\n",
    "    # Plot Power Law Lines on ax1 (they might be clipped now)\n",
    "    pl_s_line = None; pl_r_line = None\n",
    "    if not df_lines_aligned.empty:\n",
    "        pl_s_line, = ax1.plot(df_lines_aligned.index, df_lines_aligned['Support'].astype(float), color='maroon', linestyle='--', linewidth=1.5, alpha=0.8, label='PL Support (Band 0)', zorder=3)\n",
    "        pl_r_line, = ax1.plot(df_lines_aligned.index, df_lines_aligned['Resistance'].astype(float), color='indigo', linestyle='--', linewidth=1.5, alpha=0.8, label=f'PL Resistance (Band {NUM_RAINBOW_BANDS})', zorder=3)\n",
    "\n",
    "    # Plot Trade Markers on ax1\n",
    "    buy_plot = None; sell_plot = None\n",
    "    if not trades_df_plot.empty:\n",
    "        if 'price_float' not in trades_df_plot.columns: trades_df_plot['price_float'] = trades_df_plot['price'].apply(lambda x: float(x) if isinstance(x, Decimal) and not x.is_nan() else np.nan)\n",
    "        buy_markers = trades_df_plot[(trades_df_plot['type'] == 'REBALANCE_BUY') | (trades_df_plot['type'] == 'INITIAL_ALLOC_BUY')].dropna(subset=['price_float'])\n",
    "        sell_markers = trades_df_plot[trades_df_plot['type'] == 'REBALANCE_SELL'].dropna(subset=['price_float'])\n",
    "        if not buy_markers.empty: buy_plot = ax1.scatter(buy_markers['timestamp'], buy_markers['price_float'] * 1.001, label='Buy Executed', marker='^', color='lime', edgecolor='black', linewidth=0.5, s=60, alpha=1.0, zorder=5)\n",
    "        if not sell_markers.empty: sell_plot = ax1.scatter(sell_markers['timestamp'], sell_markers['price_float'] * 0.999, label='Sell Executed', marker='v', color='red', edgecolor='black', linewidth=0.5, s=60, alpha=1.0, zorder=5)\n",
    "\n",
    "    # --- REMOVED: Secondary Axis ---\n",
    "\n",
    "    # --- Final Formatting ---\n",
    "    # Collect handles manually from ax1 ONLY\n",
    "    handles_ax1, labels_ax1 = [price_line], [price_line.get_label()]\n",
    "    if pl_s_line: handles_ax1.append(pl_s_line); labels_ax1.append(pl_s_line.get_label())\n",
    "    if pl_r_line: handles_ax1.append(pl_r_line); labels_ax1.append(pl_r_line.get_label())\n",
    "    if plotted_band_line:\n",
    "        from matplotlib.lines import Line2D\n",
    "        dummy_band_line = Line2D([0], [0], color=band_line_color, linestyle=band_line_style, linewidth=band_line_width, label='Rainbow Boundaries')\n",
    "        handles_ax1.append(dummy_band_line); labels_ax1.append(dummy_band_line.get_label())\n",
    "    if buy_plot: handles_ax1.append(buy_plot); labels_ax1.append(buy_plot.get_label())\n",
    "    if sell_plot: handles_ax1.append(sell_plot); labels_ax1.append(sell_plot.get_label())\n",
    "\n",
    "    # Place legend on ax1\n",
    "    ax1.legend(handles_ax1, labels_ax1, loc='upper left', fontsize='small')\n",
    "\n",
    "    # --- Y-axis limits for ax1 were already set at the beginning ---\n",
    "\n",
    "    # --- Title ---\n",
    "    fig.suptitle(f'Backtest Analysis: Price Focus (Scaled to Data Extents) - {SYMBOL}', fontsize=14)\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.show()\n",
    "\n",
    "except NameError as ne:\n",
    "     logger.error(f\"Plotting Error: {ne}. Make sure prerequisite cells (esp. Cell 6) have been run.\")\n",
    "except ValueError as ve:\n",
    "     logger.error(f\"Plotting Error: {ve}. Check if DataFrames are empty.\")\n",
    "except Exception as plot_err:\n",
    "    logger.error(f\"Error during plotting: {plot_err}\", exc_info=True)\n",
    "\n",
    "\n",
    "# End of Cell 7 (v3.0 - Verify Input Data & Set Price Y-Limit FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a5d09-ddbe-4b10-9a7e-2a5d4a4dec5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Analyze Trades During Performance Jump (v9.3)\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "from decimal import Decimal # Import Decimal for potential formatting\n",
    "\n",
    "# --- Prerequisites ---\n",
    "if 'trades_df' not in locals() or trades_df.empty:\n",
    "    raise RuntimeError(\"trades_df is missing or empty. Run Cell 6 (Backtester) first.\")\n",
    "\n",
    "# --- Analysis Configuration ---\n",
    "# Estimate the period of the jump from the plot\n",
    "JUMP_START_DATE = pd.Timestamp(\"2023-06-15\", tz='UTC')\n",
    "JUMP_END_DATE = pd.Timestamp(\"2023-07-15\", tz='UTC')\n",
    "\n",
    "logging.info(f\"--- Analyzing Trades Around Performance Jump ({JUMP_START_DATE.date()} to {JUMP_END_DATE.date()}) ---\")\n",
    "\n",
    "# --- Filter and Display Trades ---\n",
    "try:\n",
    "    # Ensure the index is DatetimeIndex for slicing\n",
    "    if not isinstance(trades_df.index, pd.DatetimeIndex):\n",
    "        if 'timestamp' in trades_df.columns:\n",
    "            # Convert timestamp column to datetime and set as index\n",
    "            trades_df['timestamp'] = pd.to_datetime(trades_df['timestamp'])\n",
    "            trades_df_indexed = trades_df.set_index('timestamp')\n",
    "            if trades_df_indexed.index.tz is None: # Localize if needed\n",
    "                 trades_df_indexed.index = trades_df_indexed.index.tz_localize('UTC')\n",
    "            elif trades_df_indexed.index.tz != timezone.utc: # Convert if different TZ\n",
    "                 trades_df_indexed.index = trades_df_indexed.index.tz_convert('UTC')\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"'timestamp' column not found for setting index.\")\n",
    "    else:\n",
    "        # Ensure index is UTC if already DatetimeIndex\n",
    "        trades_df_indexed = trades_df.copy()\n",
    "        if trades_df_indexed.index.tz is None: trades_df_indexed.index = trades_df_indexed.index.tz_localize('UTC')\n",
    "        elif trades_df_indexed.index.tz != timezone.utc: trades_df_indexed.index = trades_df_indexed.index.tz_convert('UTC')\n",
    "\n",
    "\n",
    "    # Filter trades within the specified period\n",
    "    jump_trades = trades_df_indexed.loc[JUMP_START_DATE:JUMP_END_DATE].copy()\n",
    "\n",
    "    if jump_trades.empty:\n",
    "        logging.info(\"No trades found within the specified jump period.\")\n",
    "    else:\n",
    "        logging.info(f\"Found {len(jump_trades)} trades during the jump period:\")\n",
    "\n",
    "        # Select and format relevant columns for display\n",
    "        display_cols = ['type', 'price', 'qty', 'value', 'fee', 'buy_fill_price', 'depth_bucket', 'tp_level_pct']\n",
    "        jump_trades_display = jump_trades[[col for col in display_cols if col in jump_trades.columns]].copy() # Select existing columns\n",
    "\n",
    "        # Format numeric columns (handle potential non-Decimals if necessary)\n",
    "        for col in ['price', 'qty', 'value', 'fee', 'buy_fill_price']:\n",
    "             if col in jump_trades_display.columns:\n",
    "                 try:\n",
    "                     prec = 8 if 'qty' in col else 4\n",
    "                     # Ensure conversion from string/object if needed, then format\n",
    "                     jump_trades_display[col] = jump_trades_display[col].apply(lambda x: f\"{Decimal(str(x)):.{prec}f}\" if pd.notna(x) else 'N/A')\n",
    "                 except Exception:\n",
    "                      logging.warning(f\"Could not format column {col} as Decimal.\") # Keep original on error\n",
    "\n",
    "        # Format percentage\n",
    "        if 'tp_level_pct' in jump_trades_display.columns:\n",
    "             jump_trades_display['tp_level_pct'] = jump_trades_display['tp_level_pct'].apply(lambda x: f\"{x*100:.0f}%\" if pd.notna(x) else 'N/A')\n",
    "\n",
    "\n",
    "        print(jump_trades_display.to_string())\n",
    "\n",
    "        # Basic summary statistics for the period\n",
    "        initial_alloc_trades = jump_trades[jump_trades['type'] == 'INITIAL_ALLOC_BUY']\n",
    "        buy_trades_period = jump_trades[jump_trades['type'] == 'BUY_PL_GRID']\n",
    "        sell_trades_period = jump_trades[jump_trades['type'] == 'SELL_PL_GRID_TP']\n",
    "\n",
    "        # Convert relevant columns back to Decimal for calculation if they exist and are valid\n",
    "        try: buy_value = sum(Decimal(str(v)) for v in buy_trades_period['value'] if pd.notna(v))\n",
    "        except: buy_value = Decimal('0')\n",
    "        try: sell_value = sum(Decimal(str(v)) for v in sell_trades_period['value'] if pd.notna(v))\n",
    "        except: sell_value = Decimal('0')\n",
    "        try: fees_period = sum(Decimal(str(v)) for v in jump_trades['fee'] if pd.notna(v))\n",
    "        except: fees_period = Decimal('0')\n",
    "\n",
    "        logging.info(f\"\\nSummary for {JUMP_START_DATE.date()} to {JUMP_END_DATE.date()}:\")\n",
    "        logging.info(f\"  Buy Trades: {len(buy_trades_period)}, Total Cost (excl. fee): {-buy_value:.2f}\")\n",
    "        logging.info(f\"  Sell Trades: {len(sell_trades_period)}, Total Proceeds (excl. fee): {sell_value:.2f}\")\n",
    "        logging.info(f\"  Net Value Change (excl. fee): {buy_value + sell_value:.2f}\")\n",
    "        logging.info(f\"  Total Fees: {fees_period:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error analyzing jump trades: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "# End of Cell 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a9334-c020-42fa-93d8-ab4091bc935f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: Dynamic S/R Zone Calculation Helper Function\n",
    "\n",
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# --- Prerequisites ---\n",
    "# Needs N_PERIODS_SWING, ZONE_THRESHOLD_PERCENT_SUP, ZONE_THRESHOLD_PERCENT_RES defined\n",
    "# (These should be available if Cell 12 config was run/copied, but let's define defaults just in case)\n",
    "if 'N_PERIODS_SWING' not in locals(): N_PERIODS_SWING = 11\n",
    "if 'ZONE_THRESHOLD_PERCENT_SUP' not in locals(): ZONE_THRESHOLD_PERCENT_SUP = Decimal('0.005')\n",
    "if 'ZONE_THRESHOLD_PERCENT_RES' not in locals(): ZONE_THRESHOLD_PERCENT_RES = Decimal('0.005')\n",
    "\n",
    "# --- Helper Function Definition ---\n",
    "def calculate_dynamic_zones(data_slice, n_periods, sup_thresh_pct, res_thresh_pct):\n",
    "    \"\"\"\n",
    "    Calculates dynamic support and resistance zones based on swing points\n",
    "    within a given data slice (Pandas DataFrame).\n",
    "\n",
    "    Args:\n",
    "        data_slice (pd.DataFrame): DataFrame slice with 'High', 'Low' columns (Decimal type).\n",
    "        n_periods (int): The odd number of periods to define a swing point (e.g., 11).\n",
    "        sup_thresh_pct (Decimal): The percentage threshold to cluster support points.\n",
    "        res_thresh_pct (Decimal): The percentage threshold to cluster resistance points.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pd.DataFrame, pd.DataFrame) containing support zones and resistance zones.\n",
    "               Columns: ['zone_min_price', 'zone_max_price', 'points', 'timestamps', 'num_points']\n",
    "    \"\"\"\n",
    "    support_zones_list = []\n",
    "    resistance_zones_list = []\n",
    "\n",
    "    if len(data_slice) < n_periods:\n",
    "        # Not enough data for the lookback window\n",
    "        # logging.debug(f\"Not enough data ({len(data_slice)} < {n_periods}) to calculate dynamic zones.\")\n",
    "        return pd.DataFrame(support_zones_list), pd.DataFrame(resistance_zones_list)\n",
    "\n",
    "    half_n = n_periods // 2\n",
    "    df_sr = data_slice.copy() # Work on a copy\n",
    "    # Ensure High/Low are present\n",
    "    if not all(col in df_sr.columns for col in ['High', 'Low']):\n",
    "         logging.warning(\"Missing High/Low columns in data_slice for dynamic zone calculation.\")\n",
    "         return pd.DataFrame(support_zones_list), pd.DataFrame(resistance_zones_list)\n",
    "\n",
    "    # Initialize swing columns\n",
    "    df_sr['swing_high'] = pd.Series(index=df_sr.index, dtype=object)\n",
    "    df_sr['swing_low'] = pd.Series(index=df_sr.index, dtype=object)\n",
    "\n",
    "    # --- Calculate swing points ---\n",
    "    # Iterate through the range where a full window is available\n",
    "    for i in range(half_n, len(df_sr) - half_n):\n",
    "        idx_label = df_sr.index[i]\n",
    "        try:\n",
    "            # Check for Swing High\n",
    "            is_sh = all(df_sr['High'].iloc[i] >= df_sr['High'].iloc[i-j] for j in range(1, half_n + 1)) and \\\n",
    "                    all(df_sr['High'].iloc[i] > df_sr['High'].iloc[i+j] for j in range(1, half_n + 1))\n",
    "            if is_sh: df_sr.loc[idx_label, 'swing_high'] = df_sr['High'].iloc[i]\n",
    "\n",
    "            # Check for Swing Low\n",
    "            is_sl = all(df_sr['Low'].iloc[i] <= df_sr['Low'].iloc[i-j] for j in range(1, half_n + 1)) and \\\n",
    "                    all(df_sr['Low'].iloc[i] < df_sr['Low'].iloc[i+j] for j in range(1, half_n + 1))\n",
    "            if is_sl: df_sr.loc[idx_label, 'swing_low'] = df_sr['Low'].iloc[i]\n",
    "        except IndexError:\n",
    "            # This can happen near the edges if data_slice is shorter than expected, though the initial check should prevent it.\n",
    "            logging.warning(f\"IndexError during swing point calculation at index {i}. Check data slice length.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error calculating swing point at index {i}: {e}\")\n",
    "             continue # Skip this point\n",
    "\n",
    "    # --- Cluster Support Zones ---\n",
    "    swing_lows_series = df_sr['swing_low'].dropna()\n",
    "    current_zone_sup = None\n",
    "    if not swing_lows_series.empty:\n",
    "        try:\n",
    "            swing_lows_df = swing_lows_series.reset_index()\n",
    "            swing_lows_df.columns = ['timestamp', 'price']\n",
    "            # Ensure price is Decimal\n",
    "            swing_lows_df['price'] = swing_lows_df['price'].apply(lambda x: Decimal(str(x)))\n",
    "            swing_lows_df = swing_lows_df.sort_values(by='price').reset_index(drop=True)\n",
    "\n",
    "            current_zone_sup = {'zone_min_price': swing_lows_df['price'].iloc[0],\n",
    "                               'zone_max_price': swing_lows_df['price'].iloc[0],\n",
    "                               'points': [swing_lows_df['price'].iloc[0]],\n",
    "                               'timestamps': [swing_lows_df['timestamp'].iloc[0]]}\n",
    "\n",
    "            for i in range(1, len(swing_lows_df)):\n",
    "                cp = swing_lows_df['price'].iloc[i]; ct = swing_lows_df['timestamp'].iloc[i]\n",
    "                mtp = current_zone_sup['zone_max_price'] * (Decimal('1.0') + sup_thresh_pct)\n",
    "                if cp <= mtp:\n",
    "                    current_zone_sup['zone_max_price'] = max(current_zone_sup['zone_max_price'], cp)\n",
    "                    current_zone_sup['points'].append(cp)\n",
    "                    current_zone_sup['timestamps'].append(ct)\n",
    "                else:\n",
    "                    current_zone_sup['num_points'] = len(current_zone_sup['points'])\n",
    "                    support_zones_list.append(current_zone_sup)\n",
    "                    current_zone_sup = {'zone_min_price': cp, 'zone_max_price': cp, 'points': [cp], 'timestamps': [ct]}\n",
    "\n",
    "            if current_zone_sup is not None: # Add the last zone\n",
    "                current_zone_sup['num_points'] = len(current_zone_sup['points'])\n",
    "                support_zones_list.append(current_zone_sup)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error clustering support zones: {e}\")\n",
    "            support_zones_list = [] # Reset on error\n",
    "\n",
    "    # --- Cluster Resistance Zones ---\n",
    "    swing_highs_series = df_sr['swing_high'].dropna()\n",
    "    current_zone_res = None\n",
    "    if not swing_highs_series.empty:\n",
    "        try:\n",
    "            swing_highs_df = swing_highs_series.reset_index()\n",
    "            swing_highs_df.columns = ['timestamp', 'price']\n",
    "            # Ensure price is Decimal\n",
    "            swing_highs_df['price'] = swing_highs_df['price'].apply(lambda x: Decimal(str(x)))\n",
    "            swing_highs_df = swing_highs_df.sort_values(by='price', ascending=False).reset_index(drop=True) # Descending for highs\n",
    "\n",
    "            current_zone_res = {'zone_min_price': swing_highs_df['price'].iloc[0],\n",
    "                               'zone_max_price': swing_highs_df['price'].iloc[0],\n",
    "                               'points': [swing_highs_df['price'].iloc[0]],\n",
    "                               'timestamps': [swing_highs_df['timestamp'].iloc[0]]}\n",
    "\n",
    "            for i in range(1, len(swing_highs_df)):\n",
    "                cp = swing_highs_df['price'].iloc[i]; ct = swing_highs_df['timestamp'].iloc[i]\n",
    "                # Check if current price is within threshold of the MIN price in the current resistance zone\n",
    "                mtp = current_zone_res['zone_min_price'] * (Decimal('1.0') - res_thresh_pct)\n",
    "                if cp >= mtp:\n",
    "                    current_zone_res['zone_min_price'] = min(current_zone_res['zone_min_price'], cp)\n",
    "                    current_zone_res['points'].append(cp)\n",
    "                    current_zone_res['timestamps'].append(ct)\n",
    "                else:\n",
    "                    current_zone_res['num_points'] = len(current_zone_res['points'])\n",
    "                    resistance_zones_list.append(current_zone_res)\n",
    "                    current_zone_res = {'zone_min_price': cp, 'zone_max_price': cp, 'points': [cp], 'timestamps': [ct]}\n",
    "\n",
    "            if current_zone_res is not None: # Add the last zone\n",
    "                current_zone_res['num_points'] = len(current_zone_res['points'])\n",
    "                resistance_zones_list.append(current_zone_res)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error clustering resistance zones: {e}\")\n",
    "            resistance_zones_list = [] # Reset on error\n",
    "\n",
    "\n",
    "    sup_df = pd.DataFrame(support_zones_list)\n",
    "    res_df = pd.DataFrame(resistance_zones_list)\n",
    "    return sup_df, res_df\n",
    "\n",
    "logging.info(\"✅ Cell 4 Complete: Dynamic S/R Zone calculation function defined.\")\n",
    "\n",
    "# End of Cell 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd3974-a3e8-4aa9-adca-022dfa5134a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9 (Revised - Zigzag Trendlines): Plot Lines Connecting Significant Swing Points (N=11)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import logging\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "\n",
    "# --- Prerequisites ---\n",
    "if 'historical_data' not in locals() or historical_data.empty:\n",
    "    raise RuntimeError(\"historical_data missing or empty. Run Cell 1.\")\n",
    "if 'swing_low' not in historical_data.columns or 'swing_high' not in historical_data.columns:\n",
    "     raise RuntimeError(\"Swing point columns missing. Run Cell 2 (with desired N_PERIODS).\")\n",
    "\n",
    "# --- Configuration ---\n",
    "PLOT_POINTS = 300 # Number of data points (candles) to plot\n",
    "# N_PERIODS value used in Cell 2 (e.g., 11) is implicitly used via the data\n",
    "\n",
    "logging.info(f\"--- Plotting Zigzag Trendlines based on N={N_PERIODS} Swing Points (First {PLOT_POINTS} points) ---\")\n",
    "\n",
    "# Select data slice for plotting\n",
    "plot_data = historical_data.iloc[:PLOT_POINTS].copy()\n",
    "original_index = plot_data.index\n",
    "\n",
    "# --- Prepare Data for Plotting ---\n",
    "try:\n",
    "    # Convert relevant columns to float for plotting\n",
    "    plot_data['Close_float'] = plot_data['Close'].apply(lambda x: float(x) if isinstance(x, Decimal) else x)\n",
    "    plot_data['swing_low_float'] = plot_data['swing_low'].apply(lambda x: float(x) if isinstance(x, Decimal) else np.nan)\n",
    "    plot_data['swing_high_float'] = plot_data['swing_high'].apply(lambda x: float(x) if isinstance(x, Decimal) else np.nan)\n",
    "except Exception as e:\n",
    "    logging.error(f\"❌ Error converting Decimal to float for plotting: {e}\"); raise\n",
    "\n",
    "# --- Extract Significant Swing Points ---\n",
    "support_points = plot_data.dropna(subset=['swing_low_float'])\n",
    "resistance_points = plot_data.dropna(subset=['swing_high_float'])\n",
    "\n",
    "# --- Create Plot ---\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Plot Close price\n",
    "ax.plot(original_index, plot_data['Close_float'], label='Close Price', color='blue', linewidth=1, zorder=2)\n",
    "\n",
    "# --- Plot Zigzag Trendlines ---\n",
    "plotted_support = False\n",
    "plotted_resistance = False\n",
    "support_lines_plotted_count = 0\n",
    "resistance_lines_plotted_count = 0\n",
    "\n",
    "logging.info(\"Plotting support trendlines (connecting higher lows)...\")\n",
    "if len(support_points) >= 2:\n",
    "    for i in range(1, len(support_points)):\n",
    "        prev_point = support_points.iloc[i-1]\n",
    "        curr_point = support_points.iloc[i]\n",
    "\n",
    "        # Check if current low is higher than previous low\n",
    "        if curr_point['swing_low_float'] > prev_point['swing_low_float']:\n",
    "            # Plot line segment between these two points\n",
    "            segment_dates = [prev_point.name, curr_point.name] # .name gets the index (timestamp)\n",
    "            segment_prices = [prev_point['swing_low_float'], curr_point['swing_low_float']]\n",
    "            ax.plot(segment_dates, segment_prices, color='green', linestyle='--', linewidth=1,\n",
    "                    label='Support Trendline (Zigzag)' if not plotted_support else \"\")\n",
    "            plotted_support = True\n",
    "            support_lines_plotted_count += 1\n",
    "\n",
    "logging.info(\"Plotting resistance trendlines (connecting lower highs)...\")\n",
    "if len(resistance_points) >= 2:\n",
    "     for i in range(1, len(resistance_points)):\n",
    "        prev_point = resistance_points.iloc[i-1]\n",
    "        curr_point = resistance_points.iloc[i]\n",
    "\n",
    "        # Check if current high is lower than previous high\n",
    "        if curr_point['swing_high_float'] < prev_point['swing_high_float']:\n",
    "             # Plot line segment between these two points\n",
    "            segment_dates = [prev_point.name, curr_point.name]\n",
    "            segment_prices = [prev_point['swing_high_float'], curr_point['swing_high_float']]\n",
    "            ax.plot(segment_dates, segment_prices, color='red', linestyle='--', linewidth=1,\n",
    "                    label='Resistance Trendline (Zigzag)' if not plotted_resistance else \"\")\n",
    "            plotted_resistance = True\n",
    "            resistance_lines_plotted_count += 1\n",
    "\n",
    "\n",
    "logging.info(f\"Plotted {support_lines_plotted_count} support segments.\")\n",
    "logging.info(f\"Plotted {resistance_lines_plotted_count} resistance segments.\")\n",
    "\n",
    "# Add dummy lines for legend only if NO lines of that type were plotted\n",
    "if not plotted_support: ax.plot([], [], color='green', linestyle='--', label='Support Trendline (Zigzag)')\n",
    "if not plotted_resistance: ax.plot([], [], color='red', linestyle='--', label='Resistance Trendline (Zigzag)')\n",
    "\n",
    "\n",
    "# --- Formatting ---\n",
    "# Get N used in Cell 2 if possible (otherwise assume 11)\n",
    "n_val_display = N_PERIODS if 'N_PERIODS' in locals() else 11 # Show N=11 if N_PERIODS var not found\n",
    "ax.set_title(f'{SYMBOL} Hourly Price with Zigzag Trendlines (N={n_val_display}, First {PLOT_POINTS} points)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price (USDT)')\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend()\n",
    "fig.autofmt_xdate()\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logging.info(f\"✅ Zigzag trendline plot generated.\")\n",
    "\n",
    "# End of Cell 9 (Revised - Zigzag Trendlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f668f870-600e-4bbe-99e4-8415ec86b440",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 10 (Corrected): Plan BUY Orders based on Support Zones - Check globals()\n",
    "\n",
    "import pandas as pd\n",
    "from decimal import Decimal, ROUND_DOWN, ROUND_UP\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# --- Prerequisites ---\n",
    "if 'historical_data' not in locals() or historical_data.empty: raise RuntimeError(\"Run Cell 1\")\n",
    "if 'support_zones_df' not in locals(): raise RuntimeError(\"Run Cell 4 for support zones\")\n",
    "# *** MODIFIED: Check globals() instead of locals() ***\n",
    "if not all(v in globals() for v in ['price_tick_size_bt', 'min_qty_bt', 'qty_step_size_bt', 'min_notional_bt']):\n",
    "    raise RuntimeError(\"Filters/helpers not found in global scope. Run Cell 8.1 again.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "PLANNING_BUDGET_SIM = Decimal('500.0') # Example simulated budget\n",
    "NUM_ORDERS_TO_PLAN = 5 # Target number of buy levels\n",
    "TARGET_PRICE_IN_ZONE = 'zone_max_price' # Options: 'zone_min_price', 'zone_max_price', 'zone_mid'\n",
    "QTY_SCALE_FACTOR_BT = Decimal('1.2') # Geometric scaling factor (same as backtester)\n",
    "MIN_QTY_VALUE_MULTIPLE_BT = Decimal('1.01') # Buffer for base target value\n",
    "QUOTE_ASSET = 'USDT' # Make sure this is defined\n",
    "\n",
    "logging.info(f\"--- Planning {NUM_ORDERS_TO_PLAN} BUY Orders for {SYMBOL} using Support Zones ---\")\n",
    "logging.info(f\"Simulated Budget: {PLANNING_BUDGET_SIM:.2f} {QUOTE_ASSET}\")\n",
    "logging.info(f\"Targeting '{TARGET_PRICE_IN_ZONE}' within zones.\")\n",
    "\n",
    "# --- Get Current Price ---\n",
    "if historical_data.empty: raise ValueError(\"Historical data is empty\")\n",
    "current_price = historical_data['Close'].iloc[-1]\n",
    "logging.info(f\"Using last close price as 'current': {current_price:.4f} {QUOTE_ASSET}\")\n",
    "\n",
    "# --- Identify Relevant Support Zones ---\n",
    "relevant_zones = support_zones_df[support_zones_df['zone_max_price'] < current_price].copy()\n",
    "if relevant_zones.empty:\n",
    "    logging.warning(\"No support zones found below the current price. No orders planned.\")\n",
    "    planned_buy_orders = []\n",
    "else:\n",
    "    # Sort by price descending (closest zones first) and take top N\n",
    "    relevant_zones = relevant_zones.sort_values(by='zone_max_price', ascending=False)\n",
    "    target_zones = relevant_zones.head(NUM_ORDERS_TO_PLAN).copy()\n",
    "    if len(target_zones) < NUM_ORDERS_TO_PLAN:\n",
    "        logging.warning(f\"Found only {len(target_zones)} support zones below current price, planning for {len(target_zones)} levels.\")\n",
    "    target_zones = target_zones.sort_values(by='zone_max_price', ascending=True) # Sort deepest first for scaling calc later\n",
    "    target_zones.reset_index(inplace=True) # Reset index for easy iteration by level\n",
    "    num_actual_levels = len(target_zones)\n",
    "\n",
    "    logging.info(f\"Targeting {num_actual_levels} zones/levels below current price:\")\n",
    "    for idx, row in target_zones.iterrows():\n",
    "        logging.info(f\"  Level {idx+1} (Deepest={idx==0}): Zone {row['zone_min_price']:.4f}-{row['zone_max_price']:.4f} ({row['num_points']} pts)\")\n",
    "\n",
    "    # --- Calculate Base Target Value ---\n",
    "    closest_zone_price_target = target_zones[TARGET_PRICE_IN_ZONE].iloc[-1] # Price target in the highest (closest) zone\n",
    "    min_value_from_qty = min_qty_bt * closest_zone_price_target\n",
    "    base_target_quote_value = max(min_value_from_qty, min_notional_bt) * MIN_QTY_VALUE_MULTIPLE_BT\n",
    "    logging.info(f\"Calculated Base Target Quote Value (for scaling): {base_target_quote_value:.4f}\")\n",
    "\n",
    "    # --- Plan Orders Iteratively (Deepest First for Budget Check) ---\n",
    "    planned_buy_orders = []\n",
    "    remaining_budget = PLANNING_BUDGET_SIM\n",
    "    temp_plan_cost = Decimal('0')\n",
    "\n",
    "    for i in range(num_actual_levels):\n",
    "        zone_row = target_zones.iloc[i] # Deepest zones first due to sorting\n",
    "        order_level_display = i + 1 # Level 1 is deepest\n",
    "\n",
    "        raw_target_price = zone_row[TARGET_PRICE_IN_ZONE]\n",
    "        adjusted_target_price = adjust_price_bt(raw_target_price, price_tick_size_bt)\n",
    "\n",
    "        if adjusted_target_price is None or adjusted_target_price <= 0:\n",
    "            logging.warning(f\"  Level {order_level_display}: Invalid adjusted price {adjusted_target_price}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate target quote value using geometric scaling (deepest gets highest scale)\n",
    "        scale_exponent = i\n",
    "        target_quote_value = base_target_quote_value * (QTY_SCALE_FACTOR_BT ** scale_exponent)\n",
    "\n",
    "        # Calculate and adjust quantity\n",
    "        raw_order_qty = target_quote_value / adjusted_target_price if adjusted_target_price > 0 else Decimal('0')\n",
    "        adjusted_order_qty = adjust_qty_bt(raw_order_qty, min_qty_bt, qty_step_size_bt)\n",
    "\n",
    "        if adjusted_order_qty <= 0:\n",
    "            logging.warning(f\"  Level {order_level_display}: Adjusted quantity {adjusted_order_qty} is zero. Skipping (RawQty={raw_order_qty:.8f}, TargetVal={target_quote_value:.4f}).\")\n",
    "            continue\n",
    "\n",
    "        # Calculate notional value and check filters/budget\n",
    "        notional_value = adjusted_target_price * adjusted_order_qty\n",
    "        if notional_value < min_notional_bt:\n",
    "             logging.warning(f\"  Level {order_level_display}: Notional {notional_value:.4f} < MinNotional {min_notional_bt:.4f}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        if (remaining_budget - temp_plan_cost) < notional_value:\n",
    "            logging.warning(f\"  Level {order_level_display}: Cost {notional_value:.4f} > Remaining Budget ({remaining_budget - temp_plan_cost:.4f}). Stopping plan.\")\n",
    "            break # Stop planning if budget exceeded\n",
    "\n",
    "        # Add to potential plan\n",
    "        planned_buy_orders.append({\n",
    "            \"symbol\": SYMBOL,\n",
    "            \"level\": order_level_display, # 1 = deepest\n",
    "            \"zone_min\": zone_row['zone_min_price'],\n",
    "            \"zone_max\": zone_row['zone_max_price'],\n",
    "            \"price_target\": adjusted_target_price,\n",
    "            \"qty_target\": adjusted_order_qty,\n",
    "            \"notional_target\": notional_value\n",
    "        })\n",
    "        temp_plan_cost += notional_value\n",
    "        logging.info(f\"  Level {order_level_display}: Planned BUY {adjusted_order_qty:.8f} @ {adjusted_target_price:.4f}, Cost: {notional_value:.4f}\")\n",
    "\n",
    "    # Final budget update\n",
    "    remaining_budget -= temp_plan_cost\n",
    "    logging.info(f\"Planning complete. Total cost: {temp_plan_cost:.4f}, Remaining budget: {remaining_budget:.4f}\")\n",
    "\n",
    "\n",
    "# --- Display Final Plan ---\n",
    "print(f\"\\n--- Final BUY Order Plan ({len(planned_buy_orders)} Orders) ---\")\n",
    "if planned_buy_orders:\n",
    "    plan_df = pd.DataFrame(planned_buy_orders)\n",
    "    # Reorder levels for display (1=closest)\n",
    "    plan_df['display_level'] = plan_df['level'].rank(method='dense', ascending=False).astype(int)\n",
    "    plan_df = plan_df.sort_values('display_level')\n",
    "    # Format Decimals for printing\n",
    "    for col in ['zone_min', 'zone_max', 'price_target', 'qty_target', 'notional_target']:\n",
    "         prec = 8 if 'qty' in col else 4\n",
    "         plan_df[col] = plan_df[col].apply(lambda d: f\"{d:.{prec}f}\")\n",
    "\n",
    "    print(plan_df[['display_level', 'zone_min', 'zone_max', 'price_target', 'qty_target', 'notional_target']].to_string(index=False))\n",
    "else:\n",
    "    print(\"No BUY orders were planned.\")\n",
    "\n",
    "# End of Cell 9 (Corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee35822-461c-4e05-a8d2-01478f4a2411",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 11: Plan Take Profit SELL Order based on Resistance Zones\n",
    "\n",
    "import pandas as pd\n",
    "from decimal import Decimal, ROUND_UP # Need ROUND_UP for TP price adjustment\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# --- Prerequisites ---\n",
    "if 'resistance_zones_df' not in locals(): raise RuntimeError(\"Run Cell 6 for resistance zones\")\n",
    "if 'planned_buy_orders' not in locals(): raise RuntimeError(\"Run Cell 9 to generate a buy plan\")\n",
    "if not all(v in globals() for v in ['price_tick_size_bt', 'min_qty_bt', 'qty_step_size_bt', 'min_notional_bt']):\n",
    "    raise RuntimeError(\"Run Cell 8.1 for filters and helpers\")\n",
    "\n",
    "# --- Configuration ---\n",
    "TARGET_PRICE_IN_RES_ZONE = 'zone_min_price' # Target the bottom edge for TP\n",
    "\n",
    "logging.info(f\"--- Planning Take Profit SELL based on Resistance Zones ---\")\n",
    "\n",
    "# --- Simulate a Buy Fill ---\n",
    "# Let's assume the closest planned buy order (highest price) filled\n",
    "if not planned_buy_orders:\n",
    "    logging.warning(\"No planned buy orders from Cell 9 to simulate a fill from. Skipping TP planning.\")\n",
    "    planned_tp_sell_order = None\n",
    "else:\n",
    "    # Get the details of the highest priced buy order planned in Cell 9\n",
    "    # Note: 'planned_buy_orders' is sorted deepest=level 1, closest=last element\n",
    "    simulated_fill = planned_buy_orders[-1] # Get the last element (closest/highest price)\n",
    "    simulated_fill_price = simulated_fill['price_target']\n",
    "    simulated_fill_qty = simulated_fill['qty_target']\n",
    "    logging.info(f\"Simulating fill of BUY Level {simulated_fill['level']} (Closest): Qty={simulated_fill_qty:.8f} @ Price={simulated_fill_price:.4f}\")\n",
    "\n",
    "    # --- Identify Relevant Resistance Zones ---\n",
    "    relevant_res_zones = resistance_zones_df[resistance_zones_df['zone_min_price'] > simulated_fill_price].copy()\n",
    "    if relevant_res_zones.empty:\n",
    "        logging.warning(f\"No resistance zones found above the simulated fill price ({simulated_fill_price:.4f}). Cannot plan TP.\")\n",
    "        planned_tp_sell_order = None\n",
    "    else:\n",
    "        # Sort by price ascending (closest resistance zone first)\n",
    "        relevant_res_zones = relevant_res_zones.sort_values(by='zone_min_price', ascending=True)\n",
    "        target_res_zone = relevant_res_zones.iloc[0] # Take the first one (closest above fill price)\n",
    "\n",
    "        logging.info(f\"Targeting closest resistance zone: {target_res_zone['zone_min_price']:.4f}-{target_res_zone['zone_max_price']:.4f} ({target_res_zone['num_points']} pts)\")\n",
    "\n",
    "        # --- Calculate and Adjust TP Price ---\n",
    "        # Helper for TP price adjustment (adjust UP)\n",
    "        def adjust_tp_price_bt(price, tick_size):\n",
    "            \"\"\" Adjusts TP price UP to the nearest tick size multiple. \"\"\"\n",
    "            try:\n",
    "                if not (isinstance(price, Decimal) and isinstance(tick_size, Decimal) and tick_size > 0): return None\n",
    "                ticks = price / tick_size; adjusted_ticks = ticks.to_integral_value(rounding=ROUND_UP); return adjusted_ticks * tick_size\n",
    "            except Exception: return None\n",
    "\n",
    "        raw_tp_price = target_res_zone[TARGET_PRICE_IN_RES_ZONE]\n",
    "        adjusted_tp_price = adjust_tp_price_bt(raw_tp_price, price_tick_size_bt) # Adjust UP\n",
    "\n",
    "        if adjusted_tp_price is None or adjusted_tp_price <= simulated_fill_price:\n",
    "             logging.warning(f\"Invalid adjusted TP price ({adjusted_tp_price}) or not above fill price. Cannot plan TP.\")\n",
    "             planned_tp_sell_order = None\n",
    "        else:\n",
    "            # --- Adjust Quantity ---\n",
    "            # Sell the quantity that was filled\n",
    "            adjusted_sell_qty = adjust_qty_bt(simulated_fill_qty, min_qty_bt, qty_step_size_bt)\n",
    "\n",
    "            if adjusted_sell_qty <= 0:\n",
    "                logging.warning(f\"Adjusted SELL quantity ({adjusted_sell_qty}) is zero. Cannot plan TP.\")\n",
    "                planned_tp_sell_order = None\n",
    "            else:\n",
    "                 # --- Validate Notional ---\n",
    "                tp_notional_value = adjusted_tp_price * adjusted_sell_qty\n",
    "                if tp_notional_value < min_notional_bt:\n",
    "                    logging.warning(f\"Planned TP SELL Notional ({tp_notional_value:.4f}) < MinNotional ({min_notional_bt:.4f}). TP Order would fail.\")\n",
    "                    planned_tp_sell_order = None # Don't plan if it's guaranteed to fail filter\n",
    "                else:\n",
    "                    planned_tp_sell_order = {\n",
    "                        \"symbol\": SYMBOL,\n",
    "                        \"type\": \"SELL_TP\",\n",
    "                        \"buy_fill_price\": simulated_fill_price,\n",
    "                        \"buy_fill_qty\": simulated_fill_qty,\n",
    "                        \"target_res_zone_min\": target_res_zone['zone_min_price'],\n",
    "                        \"target_res_zone_max\": target_res_zone['zone_max_price'],\n",
    "                        \"tp_price_target\": adjusted_tp_price,\n",
    "                        \"tp_qty_target\": adjusted_sell_qty,\n",
    "                        \"tp_notional_target\": tp_notional_value\n",
    "                    }\n",
    "                    logging.info(f\"Successfully planned TP SELL order: Sell {adjusted_sell_qty:.8f} @ {adjusted_tp_price:.4f}\")\n",
    "\n",
    "\n",
    "# --- Display Final TP Plan ---\n",
    "print(f\"\\n--- Final Take Profit SELL Order Plan (Based on simulated fill) ---\")\n",
    "if planned_tp_sell_order:\n",
    "    # Format Decimals for printing\n",
    "    tp_plan_series = pd.Series(planned_tp_sell_order)\n",
    "    for col in tp_plan_series.index:\n",
    "        val = tp_plan_series[col]\n",
    "        if isinstance(val, Decimal):\n",
    "             prec = 8 if 'qty' in col else 4\n",
    "             tp_plan_series[col] = f\"{val:.{prec}f}\"\n",
    "    print(tp_plan_series)\n",
    "\n",
    "else:\n",
    "    print(\"No Take Profit SELL order was planned.\")\n",
    "\n",
    "\n",
    "# End of Cell 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1260bda2-51b1-47e0-80f1-ced773ae4be8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell X (Originally Cell 12, v9.0 - PL Filtered Dyn S/R Entry, Scaled PL% TP, Grid Abandon): Backtesting Engine\n",
    "# Ensure this code is placed in the correct sequence after its dependencies (Cells 1, 3, 2, 4, 5)\n",
    "\n",
    "import pandas as pd\n",
    "from decimal import Decimal, ROUND_DOWN, ROUND_UP, InvalidOperation, getcontext\n",
    "import logging\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from tqdm.notebook import tqdm # Progress bar\n",
    "# pandas_ta might be needed if ATR is used anywhere, kept for safety\n",
    "try:\n",
    "    import pandas_ta as ta\n",
    "except ImportError:\n",
    "    logging.warning(\"pandas_ta not found, ATR features might fail if used.\")\n",
    "\n",
    "\n",
    "# --- Prerequisites ---\\\n",
    "# Ensure these are available from preceding cells (using the new numbering)\n",
    "if 'historical_data_test' not in locals() or historical_data_test.empty: raise RuntimeError(\"Run Cell 2 for historical_data_test\")\n",
    "if 'data_for_sr_calc' not in locals() or not isinstance(data_for_sr_calc, dict): raise RuntimeError(\"Run Cell 2 for data_for_sr_calc\")\n",
    "if not all(v in globals() for v in ['price_tick_size_bt', 'min_qty_bt', 'qty_step_size_bt', 'min_notional_bt', 'adjust_price_bt', 'adjust_qty_bt', 'adjust_tp_price_bt']): raise RuntimeError(\"Run Cell 3 for filters/helpers\")\n",
    "if 'SYMBOL' not in locals(): raise RuntimeError(\"Run Cell 1 for SYMBOL.\")\n",
    "if 'df_lines' not in locals() or df_lines.empty: raise RuntimeError(\"Run Cell 5 for df_lines (Power Law).\")\n",
    "if 'calculate_dynamic_zones' not in globals(): raise RuntimeError(\"Run Cell 4 for calculate_dynamic_zones function.\")\n",
    "\n",
    "# --- Configuration (from original Cell 12) ---\n",
    "getcontext().prec = 18\n",
    "logger = logging.getLogger(__name__)\n",
    "# Ensure INFO level logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', stream=sys.stdout, force=True)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "STARTING_QUOTE_BALANCE = Decimal('1000.0'); STARTING_BASE_BALANCE = Decimal('0.0')\n",
    "NUM_ORDERS_PER_TF_LEVEL = 3\n",
    "QTY_SCALE_FACTOR_BT = Decimal('1.2'); MIN_QTY_VALUE_MULTIPLE_BT = Decimal('1.01')\n",
    "TARGET_PRICE_IN_SUP_ZONE = 'zone_max_price' # Target top of dynamic support zone\n",
    "QUOTE_ASSET = 'USDT'; BASE_ASSET = SYMBOL.replace(QUOTE_ASSET, '')\n",
    "FEE_RATE = Decimal('0.001'); ROLLING_WINDOW_FOR_SR = 500\n",
    "N_PERIODS_SWING = 11 # Make sure this matches Cell 4 if modified\n",
    "ZONE_THRESHOLD_PERCENT_SUP = Decimal('0.005')\n",
    "ZONE_THRESHOLD_PERCENT_RES = Decimal('0.005')\n",
    "ATR_PERIOD = 14 # Needed for Grid Abandonment (if used)\n",
    "\n",
    "TF_SETTINGS = {\n",
    "    '1h': {'replan_interval': 24, 'budget_alloc': Decimal('0.20'), 'num_orders': NUM_ORDERS_PER_TF_LEVEL},\n",
    "    '4h': {'replan_interval': 24*3, 'budget_alloc': Decimal('0.30'), 'num_orders': NUM_ORDERS_PER_TF_LEVEL},\n",
    "    '1d': {'replan_interval': 24*7, 'budget_alloc': Decimal('0.50'), 'num_orders': NUM_ORDERS_PER_TF_LEVEL}\n",
    "}\n",
    "total_alloc = sum(v['budget_alloc'] for v in TF_SETTINGS.values())\n",
    "if total_alloc != Decimal('1.0'): logging.warning(f\"Budget allocations sum to {total_alloc}, not 1.0!\")\n",
    "\n",
    "# --- Grid Abandonment Config ---\n",
    "ABANDON_ATR_MULT = Decimal('3.0') # Abandon grid if price > highest_bid + 3*ATR\n",
    "BUFFER_BELOW_PL = Decimal('0.001') # Allow dynamic zones slightly (0.1%) below PL Support\n",
    "\n",
    "# --- TP Config: Scaled Percentage levels within the PL Channel ---\n",
    "TP_INITIAL_TARGET_PCT = {'1h': Decimal('0.25'), '4h': Decimal('0.50'), '1d': Decimal('0.75')}\n",
    "TP_FALLBACK_PERCENTAGES = [Decimal('0.25'), Decimal('0.50'), Decimal('0.75'), Decimal('1.00')]\n",
    "TP_MIN_PROFIT_PERCENT = Decimal('0.005')\n",
    "\n",
    "logging.info(f\"--- Starting Backtest Simulation v9.0 for {SYMBOL} (PL Filtered Dyn S/R Entry, Scaled PL% TP, Grid Abandon) ---\") # v9.0\n",
    "logging.info(f\"Exit: Scaled PL Channel % Target (Initial: {TP_INITIAL_TARGET_PCT}, Min Profit: {TP_MIN_PROFIT_PERCENT*100}%)\\\")\")\n",
    "logging.info(f\"Entry: Multi-TF Dynamic S/R Zones[{ROLLING_WINDOW_FOR_SR}] filtered by PL Support (floor buffer {BUFFER_BELOW_PL*100:.1f}%). Replan: { {tf: s['replan_interval'] for tf, s in TF_SETTINGS.items()} }\\\")\")\n",
    "logging.info(f\"Grid Abandonment: Enabled for 1h, 4h (Threshold: Price > Highest Bid + {ABANDON_ATR_MULT} * ATR)\\\")\")\n",
    "\n",
    "# --- Simulation State Initialization ---\\\n",
    "quote_balance = STARTING_QUOTE_BALANCE; base_balance = STARTING_BASE_BALANCE\n",
    "open_buy_orders = {'1h': [], '4h': [], '1d': []}; open_sell_orders = []\n",
    "trade_log = []; portfolio_history = []\n",
    "total_fees_paid = Decimal('0.0')\n",
    "position_open = False; position_entry_price = None; position_qty = Decimal('0'); position_entry_tf = None\n",
    "last_replan_candle = {'1h': 0, '4h': 0, '1d': 0}\n",
    "current_zones = {'1h': {'support': pd.DataFrame(), 'resistance': pd.DataFrame()}, '4h': {'support': pd.DataFrame(), 'resistance': pd.DataFrame()}, '1d': {'support': pd.DataFrame(), 'resistance': pd.DataFrame()}}\n",
    "abandon_counts = {'1h': 0, '4h': 0} # Counter for abandoned grids\n",
    "\n",
    "# --- HODL Calculation Setup ---\\\n",
    "initial_hodl_price = historical_data_test['Close'].iloc[0]\n",
    "hodl_base_qty = (STARTING_QUOTE_BALANCE / initial_hodl_price) if initial_hodl_price > 0 else Decimal('0')\n",
    "hodl_portfolio_history = []\n",
    "logging.info(f\"Initial HODL Buy (adjusted): {hodl_base_qty:.8f} {BASE_ASSET} @ {initial_hodl_price:.2f}\")\n",
    "\n",
    "# --- Power Law Access ---\n",
    "# Make df_lines index timezone-aware if needed (should be done in Cell 5)\n",
    "if df_lines.index.tz is None: df_lines.index = df_lines.index.tz_localize('UTC')\n",
    "elif df_lines.index.tz != timezone.utc: df_lines.index = df_lines.index.tz_convert('UTC')\n",
    "power_law_support = df_lines['Support']\n",
    "power_law_resistance = df_lines['Resistance']\n",
    "\n",
    "# --- Simulation Loop ---\\\n",
    "logging.info(f\"Starting simulation loop v9.0 through {len(historical_data_test)} candles...\")\n",
    "candle_count = 0\n",
    "\n",
    "for timestamp, candle_data in tqdm(historical_data_test.iterrows(), total=len(historical_data_test), desc=\"MTF Backtest (v9.0 Dyn S/R)\"):\n",
    "    candle_count += 1\n",
    "    current_high = candle_data['High']; current_low = candle_data['Low']; current_close = candle_data['Close']\n",
    "    timestamp_utc = timestamp\n",
    "\n",
    "    # --- 0. Check for Replan Intervals & Calculate Dynamic Zones ---\\\n",
    "    should_plan_grid = {}\n",
    "    for tf, settings in TF_SETTINGS.items():\n",
    "        if candle_count == 1 or (candle_count - last_replan_candle[tf]) >= settings['replan_interval']:\n",
    "            tf_df = data_for_sr_calc.get(tf) # Use .get() for safety\n",
    "            if tf_df is None or tf_df.empty:\n",
    "                 logger.warning(f\"No data found in data_for_sr_calc for TF {tf} @ {timestamp_utc}. Skipping zone recalc.\")\n",
    "                 current_zones[tf]['support'], current_zones[tf]['resistance'] = pd.DataFrame(), pd.DataFrame()\n",
    "                 continue # Skip this TF if data is missing\n",
    "\n",
    "            try:\n",
    "                 # Find the index in the specific TF dataframe corresponding to the current backtest timestamp\n",
    "                 end_ts_for_slice = tf_df.index.asof(timestamp_utc)\n",
    "                 if pd.isna(end_ts_for_slice):\n",
    "                     # If exact timestamp not found, try finding the closest preceding one\n",
    "                     try:\n",
    "                         loc = tf_df.index.get_loc(timestamp_utc, method='ffill')\n",
    "                         end_ts_for_slice = tf_df.index[loc]\n",
    "                         logger.debug(f\"Using ffill timestamp {end_ts_for_slice} for {tf} data slice @ {timestamp_utc}\")\n",
    "                     except KeyError:\n",
    "                          raise KeyError(f\"TS {timestamp_utc} not found in {tf} index (asof/ffill).\")\n",
    "\n",
    "                 idx_loc_in_tf = tf_df.index.get_loc(end_ts_for_slice)\n",
    "                 data_end_index_tf = idx_loc_in_tf + 1\n",
    "                 data_start_index_tf = max(0, data_end_index_tf - ROLLING_WINDOW_FOR_SR)\n",
    "                 data_slice_tf = tf_df.iloc[data_start_index_tf:data_end_index_tf]\n",
    "\n",
    "                 if len(data_slice_tf) >= N_PERIODS_SWING:\n",
    "                     # Ensure data has correct types before calculating zones\n",
    "                     if not all(dtype == 'object' or np.issubdtype(dtype, np.number) for dtype in data_slice_tf[['High', 'Low']].dtypes):\n",
    "                         data_slice_tf = data_slice_tf.copy() # Avoid SettingWithCopyWarning\n",
    "                         for col in ['High', 'Low']:\n",
    "                            data_slice_tf[col] = data_slice_tf[col].apply(lambda x: Decimal(str(x)) if not isinstance(x, Decimal) else x)\n",
    "\n",
    "                     sup_zones, res_zones = calculate_dynamic_zones(data_slice_tf, N_PERIODS_SWING, ZONE_THRESHOLD_PERCENT_SUP, ZONE_THRESHOLD_PERCENT_RES)\n",
    "                     current_zones[tf]['support'] = sup_zones; current_zones[tf]['resistance'] = res_zones\n",
    "                     logger.debug(f\"Recalculated {tf} zones @ {timestamp_utc}: {len(sup_zones)} support, {len(res_zones)} resistance.\")\n",
    "                 else:\n",
    "                     current_zones[tf]['support'], current_zones[tf]['resistance'] = pd.DataFrame(), pd.DataFrame()\n",
    "                     logger.debug(f\"Not enough data ({len(data_slice_tf)}<{N_PERIODS_SWING}) for {tf} zones @ {timestamp_utc}\")\n",
    "\n",
    "            except KeyError as ke: logger.warning(f\"TS {timestamp_utc} align error in {tf} ({ke}), skipping zone recalc.\"); current_zones[tf]['support'], current_zones[tf]['resistance'] = pd.DataFrame(), pd.DataFrame()\n",
    "            except Exception as e_sr: logger.error(f\"Error calc {tf} zones @ {timestamp_utc}: {e_sr}\"); current_zones[tf]['support'], current_zones[tf]['resistance'] = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "            if not position_open: should_plan_grid[tf] = True # Only flag for planning if not in position\n",
    "            last_replan_candle[tf] = candle_count\n",
    "\n",
    "    # --- Grid Abandonment Check (using ATR) ---\\\n",
    "    if not position_open:\n",
    "        for tf in ['1h', '4h']:\n",
    "            if open_buy_orders[tf]:\n",
    "                highest_bid_price = max(order['price'] for order in open_buy_orders[tf])\n",
    "                atr_col_suffix = '' if tf == '1h' else f'_{tf}'\n",
    "                atr_col = f'ATR_{ATR_PERIOD}{atr_col_suffix}'\n",
    "                current_atr = candle_data.get(atr_col, None)\n",
    "\n",
    "                if current_atr and isinstance(current_atr, Decimal) and current_atr > 0:\n",
    "                    abandon_threshold = highest_bid_price + (ABANDON_ATR_MULT * current_atr)\n",
    "                    if current_close > abandon_threshold:\n",
    "                        logging.info(f\"[{timestamp_utc}] Abandoning stale {tf} grid. Price {current_close:.2f} > threshold {abandon_threshold:.2f}\")\n",
    "                        open_buy_orders[tf] = []\n",
    "                        should_plan_grid[tf] = True # Allow replan\n",
    "                        abandon_counts[tf] += 1\n",
    "                # else: logger.debug(f\"Cannot check {tf} grid abandonment @ {timestamp_utc}: Invalid ATR {current_atr}\")\n",
    "\n",
    "\n",
    "    # --- 1. Check Fills SELL (Scaled PL Channel % TP) ---\\\n",
    "    if position_open and open_sell_orders:\n",
    "        sell_order = open_sell_orders[0]\n",
    "        if current_high >= sell_order['price']:\n",
    "            fill_price = sell_order['price']; fill_qty = sell_order['qty']\n",
    "            # Ensure we don't sell more than we have (rounding errors)\n",
    "            fill_qty = min(fill_qty, base_balance)\n",
    "            if fill_qty <= 0: # If base balance is zero or negative somehow\n",
    "                logger.warning(f\"[{timestamp_utc}] Attempted SELL TP fill with zero/negative qty. Resetting position.\")\n",
    "                position_open = False; position_entry_price = None; position_qty = Decimal('0'); position_entry_tf = None; open_sell_orders = []; base_balance = Decimal('0')\n",
    "            else:\n",
    "                gross_proceeds = fill_price * fill_qty; fee = gross_proceeds * FEE_RATE\n",
    "                net_proceeds = gross_proceeds - fee; total_fees_paid += fee\n",
    "                quote_balance += net_proceeds; base_balance -= fill_qty # Should bring base near zero\n",
    "                trade_log.append({'timestamp': timestamp_utc, 'type': f\"SELL_SCALED_PL_TP_{sell_order['entry_tf']}\", 'price': fill_price, 'qty': fill_qty, 'value': net_proceeds, 'fee': fee, 'buy_fill_price': sell_order['buy_fill_price'], 'tp_level_pct': sell_order.get('tp_level_pct')})\n",
    "                logging.info(f\"[{timestamp_utc}] +++ SELL_SCALED_PL_TP Filled ({sell_order['entry_tf']} @ {sell_order['buy_fill_price']:.2f}) TP @ {fill_price:.2f} Qty {fill_qty:.8f} +++\")\n",
    "                position_open = False; position_entry_price = None; position_qty = Decimal('0'); position_entry_tf = None\n",
    "                open_sell_orders = []\n",
    "                base_balance = max(Decimal('0'), base_balance) # Ensure base doesn't go negative due to rounding\n",
    "                # Flag all TFs for potential replan after exiting position\n",
    "                for tf_replan in TF_SETTINGS.keys(): should_plan_grid[tf_replan] = True\n",
    "                last_replan_candle = {tf_replan: candle_count for tf_replan in TF_SETTINGS.keys()}\n",
    "\n",
    "\n",
    "    # --- 2. Check Fills BUY ---\n",
    "    if not position_open:\n",
    "        all_potential_buys = []\n",
    "        for tf_key, buy_list in open_buy_orders.items():\n",
    "             for order in buy_list: all_potential_buys.append({**order, 'tf': tf_key})\n",
    "        # Sort all potential buys across all TFs by price (highest first)\n",
    "        all_potential_buys.sort(key=lambda x: x['price'], reverse=True)\n",
    "\n",
    "        for i, buy_order in enumerate(all_potential_buys):\n",
    "             if current_low <= buy_order['price']:\n",
    "                 fill_price = buy_order['price']; fill_qty = buy_order['qty']; cost = buy_order['notional']; fee = cost * FEE_RATE; total_cost_with_fee = cost + fee\n",
    "                 if quote_balance >= total_cost_with_fee:\n",
    "                     total_fees_paid += fee; quote_balance -= total_cost_with_fee; base_balance += fill_qty\n",
    "                     trade_log.append({'timestamp': timestamp_utc, 'type': f\"BUY_{buy_order['tf']}\", 'price': fill_price, 'qty': fill_qty, 'value': -cost, 'fee': fee, 'level': buy_order['level']})\n",
    "                     logging.info(f\"[{timestamp_utc}] +++ BUY Filled ({buy_order['tf']} Lvl {buy_order['level']}) @ {fill_price:.2f} Qty {fill_qty:.8f} +++\")\n",
    "\n",
    "                     # --- Enter Position ---\n",
    "                     position_open = True; position_entry_price = fill_price; position_qty = fill_qty; position_entry_tf = buy_order['tf']\n",
    "                     open_buy_orders = {'1h': [], '4h': [], '1d': []} # Clear ALL Buy orders from all TFs\n",
    "                     open_sell_orders = [] # Clear any previous TP attempts\n",
    "\n",
    "                     # --- Plan Scaled PL Channel % TP ---\n",
    "                     try:\n",
    "                         pl_s_raw = Decimal(str(power_law_support.asof(timestamp_utc)))\n",
    "                         pl_r_raw = Decimal(str(power_law_resistance.asof(timestamp_utc)))\n",
    "                         if pl_s_raw and pl_r_raw and pl_s_raw > 0 and pl_r_raw > pl_s_raw:\n",
    "                             log_pl_s = pl_s_raw.ln(); log_pl_r = pl_r_raw.ln(); log_channel_height = log_pl_r - log_pl_s\n",
    "                             if log_channel_height <= 0: raise ValueError(\"Log channel height non-positive for TP calc.\")\n",
    "\n",
    "                             min_tp_target_price = position_entry_price * (Decimal('1.0') + TP_MIN_PROFIT_PERCENT)\n",
    "                             chosen_tp_price = None; chosen_tp_pct = None\n",
    "                             # Determine starting TP level based on entry TF\n",
    "                             initial_target_pct = TP_INITIAL_TARGET_PCT.get(position_entry_tf, TP_FALLBACK_PERCENTAGES[0])\n",
    "                             try: start_index = TP_FALLBACK_PERCENTAGES.index(initial_target_pct)\n",
    "                             except ValueError: start_index = 0\n",
    "\n",
    "                             # Iterate through potential TP levels starting from the initial target\n",
    "                             for tp_pct in TP_FALLBACK_PERCENTAGES[start_index:]:\n",
    "                                 raw_tp_price = Decimal(math.exp(float(log_pl_s + tp_pct * log_channel_height)))\n",
    "                                 # Check if this level is sufficiently above the entry price\n",
    "                                 if raw_tp_price > min_tp_target_price:\n",
    "                                     chosen_tp_price = raw_tp_price\n",
    "                                     chosen_tp_pct = tp_pct\n",
    "                                     break # Use the first valid level found\n",
    "\n",
    "                             if chosen_tp_price:\n",
    "                                 adjusted_tp_price = adjust_tp_price_bt(chosen_tp_price, price_tick_size_bt)\n",
    "                                 # Final check: Ensure adjusted TP is still above entry price\n",
    "                                 if adjusted_tp_price and adjusted_tp_price > position_entry_price:\n",
    "                                     adjusted_sell_qty = adjust_qty_bt(position_qty, min_qty_bt, qty_step_size_bt)\n",
    "                                     # Ensure sell qty > 0 (handles case where position_qty was below min_qty)\n",
    "                                     if adjusted_sell_qty > 0:\n",
    "                                         tp_notional_value = adjusted_tp_price * adjusted_sell_qty\n",
    "                                         if tp_notional_value >= min_notional_bt:\n",
    "                                             open_sell_orders = [{'price': adjusted_tp_price, 'qty': adjusted_sell_qty,'buy_fill_price': position_entry_price, 'entry_tf': position_entry_tf,'tp_level_pct': chosen_tp_pct}]\n",
    "                                             logging.info(f\"  Scaled PL % TP Placed ({position_entry_tf}): Target Lvl {chosen_tp_pct*100:.0f}%, Sell {adjusted_sell_qty:.8f} @ {adjusted_tp_price:.4f}\")\n",
    "                                         else: logger.warning(f\"  TP Notional {tp_notional_value:.4f} < MinNotional {min_notional_bt}. TP order invalid.\")\n",
    "                                     else: logger.warning(f\"  Adjusted TP Sell Qty is zero ({position_qty=}). Cannot place TP.\")\n",
    "                                 else: logger.warning(f\"  Adjusted TP price {adjusted_tp_price} not valid or not above entry {position_entry_price}. No TP placed.\")\n",
    "                             else: logger.warning(f\"  No suitable PL Channel TP level found above min profit threshold {min_tp_target_price:.4f}. No TP placed.\")\n",
    "                         else: logger.warning(f\"  Invalid PL bounds for TP calc: S={pl_s_raw}, R={pl_r_raw}. No TP placed.\")\n",
    "                     except (KeyError, TypeError, ValueError, InvalidOperation, OverflowError) as pl_tp_err:\n",
    "                         logger.error(f\"  Error calculating/placing Scaled PL % TP @ {timestamp_utc}: {pl_tp_err}. No TP placed.\")\n",
    "\n",
    "                     # Break from buy order check loop once a position is entered\n",
    "                     break\n",
    "                 # else: logger.debug(f\"[{timestamp_utc}] BUY Fill Skipped (Insufficient Quote {quote_balance:.2f} < {total_cost_with_fee:.2f}) @ {buy_order['price']:.2f}\")\n",
    "\n",
    "\n",
    "    # --- 3. Plan New BUY Grids (Using Dynamic S/R Zones filtered by PL Support) ---\\\n",
    "    if not position_open: # Only plan if not already in a position\n",
    "        for tf, settings in TF_SETTINGS.items():\n",
    "             if should_plan_grid.get(tf, False): # Check if flagged for replan\n",
    "                 open_buy_orders[tf] = [] # Clear previous orders for this TF\n",
    "                 tf_budget = quote_balance * settings['budget_alloc'] # Allocate budget dynamically\n",
    "                 tf_planned_orders = []\n",
    "                 temp_plan_cost = Decimal('0')\n",
    "                 num_to_place = settings['num_orders']\n",
    "                 # Calculate base target value considering min qty/notional at current price\n",
    "                 min_value_from_qty = min_qty_bt * current_close\n",
    "                 base_target_quote_value = max(min_value_from_qty, min_notional_bt) * MIN_QTY_VALUE_MULTIPLE_BT\n",
    "\n",
    "                 try:\n",
    "                     pl_support_now = Decimal(str(power_law_support.asof(timestamp_utc)))\n",
    "                     price_floor = pl_support_now * (Decimal('1.0') - BUFFER_BELOW_PL)\n",
    "                 except (KeyError, TypeError, ValueError, InvalidOperation) as pl_err:\n",
    "                     logger.warning(f\"  Could not get PL Support for {tf} grid filter @ {timestamp_utc}: {pl_err}. Using 0.\")\n",
    "                     price_floor = Decimal('0')\n",
    "\n",
    "                 tf_support_zones = current_zones[tf]['support']\n",
    "                 if not tf_support_zones.empty:\n",
    "                     relevant_zones = tf_support_zones[\n",
    "                         (tf_support_zones['zone_max_price'] < current_close) &\n",
    "                         (tf_support_zones['zone_min_price'] >= price_floor) # Filter above PL floor\n",
    "                     ].copy()\n",
    "\n",
    "                     if not relevant_zones.empty:\n",
    "                         # Sort closest first, take N, then sort deepest first for scaling calc\n",
    "                         target_zones = relevant_zones.sort_values(by='zone_max_price', ascending=False).head(num_to_place).sort_values(by='zone_max_price', ascending=True)\n",
    "                         target_zones.reset_index(inplace=True); num_actual_levels = len(target_zones)\n",
    "\n",
    "                         if num_actual_levels > 0:\n",
    "                             # Recalculate base target based on the *closest filtered* zone's target price\n",
    "                             closest_filtered_zone_price_target = target_zones[TARGET_PRICE_IN_SUP_ZONE].iloc[-1]\n",
    "                             min_value_from_qty_filtered = min_qty_bt * closest_filtered_zone_price_target\n",
    "                             base_target_quote_value = max(min_value_from_qty_filtered, min_notional_bt) * MIN_QTY_VALUE_MULTIPLE_BT\n",
    "                             logger.debug(f\"  Recalculated base target value for {tf}: {base_target_quote_value:.4f}\")\n",
    "\n",
    "                             for i in range(num_actual_levels):\n",
    "                                 zone_row = target_zones.iloc[i]; order_level_display = i + 1 # Level 1 = deepest\n",
    "                                 raw_target_price = zone_row[TARGET_PRICE_IN_SUP_ZONE]\n",
    "                                 adjusted_target_price = adjust_price_bt(raw_target_price, price_tick_size_bt)\n",
    "                                 if adjusted_target_price and adjusted_target_price > 0:\n",
    "                                     # Geometric scaling (deepest gets highest scale)\n",
    "                                     scale_exponent = i; target_quote_value = base_target_quote_value * (QTY_SCALE_FACTOR_BT ** scale_exponent)\n",
    "                                     raw_order_qty = target_quote_value / adjusted_target_price; adjusted_order_qty = adjust_qty_bt(raw_order_qty, min_qty_bt, qty_step_size_bt)\n",
    "                                     if adjusted_order_qty > 0:\n",
    "                                         notional_value = adjusted_target_price * adjusted_order_qty\n",
    "                                         if notional_value >= min_notional_bt:\n",
    "                                             # Check against dynamic TF budget\n",
    "                                             if (tf_budget - temp_plan_cost) >= notional_value:\n",
    "                                                 tf_planned_orders.append({\"price\": adjusted_target_price, \"qty\": adjusted_order_qty, \"notional\": notional_value, \"level\": order_level_display})\n",
    "                                                 temp_plan_cost += notional_value\n",
    "                                                 logger.debug(f\"  Planned BUY ({tf} Lvl {order_level_display}): Qty {adjusted_order_qty:.8f} @ {adjusted_target_price:.4f}, Cost: {notional_value:.4f}\")\n",
    "                                             else:\n",
    "                                                logger.debug(f\"  Budget Exceeded for {tf} @ Lvl {order_level_display}. Stop planning for TF.\")\n",
    "                                                break # Stop planning for this TF if budget exceeded\n",
    "                                         # else: logger.debug(f\"  {tf} Lvl {order_level_display} Notional {notional_value:.4f} < Min {min_notional_bt}. Skip.\")\n",
    "                                     # else: logger.debug(f\"  {tf} Lvl {order_level_display} Adjusted Qty is zero. Skip.\")\n",
    "                                 # else: logger.debug(f\"  {tf} Lvl {order_level_display} Adjusted Price invalid. Skip.\")\n",
    "                             # Add planned orders for this TF to the main list\n",
    "                             open_buy_orders[tf] = tf_planned_orders\n",
    "                             logger.info(f\"  Planned {len(tf_planned_orders)} BUY orders for {tf}. Total cost: {temp_plan_cost:.4f}/{tf_budget:.4f}\")\n",
    "                         # else: logger.debug(f\"  No dynamic zones survived filtering for {tf} @ {timestamp_utc}.\")\n",
    "                     # else: logger.debug(f\"  No relevant dynamic zones calculated or found below price for {tf} @ {timestamp_utc}.\")\n",
    "                 # else: logger.debug(f\"  No dynamic support zones available for {tf} @ {timestamp_utc}.\")\n",
    "\n",
    "\n",
    "    # --- 4. Record Values ---\\\n",
    "    current_portfolio_value = quote_balance + (base_balance * current_close)\n",
    "    portfolio_history.append({'timestamp': timestamp_utc, 'portfolio_value': current_portfolio_value, 'quote': quote_balance, 'base': base_balance})\n",
    "    current_hodl_value = hodl_base_qty * current_close\n",
    "    hodl_portfolio_history.append({'timestamp': timestamp_utc, 'hodl_value': current_hodl_value})\n",
    "\n",
    "# --- Loop End ---\\\n",
    "logging.info(f\"--- Simulation Loop Finished ---\")\n",
    "\n",
    "# --- Final Portfolio Calculation & Performance Metrics ---\\\n",
    "final_quote_balance = quote_balance; final_base_balance = base_balance\n",
    "last_close_price = historical_data_test['Close'].iloc[-1] if not historical_data_test.empty else Decimal('0')\n",
    "final_portfolio_value = final_quote_balance + (final_base_balance * last_close_price)\n",
    "final_hodl_value = hodl_base_qty * last_close_price\n",
    "\n",
    "print(\"\\n--- Backtest Results (v9.0 - PL Filtered Dyn S/R Entry, Scaled PL% TP, Grid Abandon) ---\")\n",
    "print(f\"Simulation Period (Adjusted): {historical_data_test.index.min()} to {historical_data_test.index.max()}\")\n",
    "print(f\"S/R Zone Window: Last {ROLLING_WINDOW_FOR_SR} candles\")\n",
    "print(f\"TF Settings: {TF_SETTINGS}\")\n",
    "print(f\"Entry Logic: Dynamic S/R Zones filtered by PL Support Floor (buffer {BUFFER_BELOW_PL*100:.1f}%). Replan: { {tf: s['replan_interval'] for tf, s in TF_SETTINGS.items()} }\\\")\")\n",
    "print(f\"Grid Abandonment: Enabled for 1h, 4h (Threshold: {ABANDON_ATR_MULT} ATR)\\\")\")\n",
    "print(f\"Exit: Scaled PL Channel % Target (Initial: {TP_INITIAL_TARGET_PCT}, Fallback: {TP_FALLBACK_PERCENTAGES}, Min Profit: {TP_MIN_PROFIT_PERCENT*100}%)\\\")\")\n",
    "print(f\"Fee Rate Applied: {FEE_RATE*100}%\\\\n\")\n",
    "print(f\"Initial Portfolio Value: {STARTING_QUOTE_BALANCE:.2f} {QUOTE_ASSET}\")\n",
    "print(f\"Final Portfolio Value (Strategy): {final_portfolio_value:.2f} {QUOTE_ASSET}\")\n",
    "total_profit_loss = final_portfolio_value - STARTING_QUOTE_BALANCE; total_profit_loss_percent = (total_profit_loss / STARTING_QUOTE_BALANCE) * 100 if STARTING_QUOTE_BALANCE > 0 else Decimal('0')\n",
    "print(f\"Total Profit/Loss (Strategy): {total_profit_loss:.2f} {QUOTE_ASSET} ({total_profit_loss_percent:.2f}%)\\\"\")\n",
    "print(f\"Final Portfolio Value (HODL): {final_hodl_value:.2f} {QUOTE_ASSET}\")\n",
    "hodl_profit_loss = final_hodl_value - STARTING_QUOTE_BALANCE; hodl_profit_loss_percent = (hodl_profit_loss / STARTING_QUOTE_BALANCE) * 100 if STARTING_QUOTE_BALANCE > 0 else Decimal('0')\n",
    "print(f\"Total Profit/Loss (HODL): {hodl_profit_loss:.2f} {QUOTE_ASSET} ({hodl_profit_loss_percent:.2f}%)\\\"\")\n",
    "\n",
    "trades_df = pd.DataFrame(trade_log)\n",
    "print(f\"\\nTotal Trades Executed (Strategy): {len(trades_df)}\")\n",
    "total_buys = 0; total_sells = 0\n",
    "for tf_key in TF_SETTINGS.keys():\n",
    "    buys = trades_df[trades_df['type'] == f'BUY_{tf_key}'] if not trades_df.empty else pd.DataFrame()\n",
    "    sells = trades_df[trades_df['type'] == f'SELL_SCALED_PL_TP_{tf_key}'] if not trades_df.empty else pd.DataFrame()\n",
    "    total_buys += len(buys); total_sells += len(sells)\n",
    "    print(f\"  {tf_key} Entry Trades: {len(buys)} Buys, {len(sells)} Sells\")\n",
    "    if not sells.empty and 'buy_fill_price' in sells.columns:\n",
    "       try:\n",
    "           # Ensure comparison is between numeric types\n",
    "           wins = sells[sells['price'].astype(float) > sells['buy_fill_price'].astype(float)]\n",
    "           win_rate = (len(wins) / len(sells)) * 100 if len(sells) > 0 else 0\n",
    "           print(f\"    {tf_key} Scaled PL % TP Win Rate: {win_rate:.2f}% ({len(wins)} wins / {len(sells)} exits)\")\n",
    "       except Exception as e:\n",
    "           print(f\"    Error calculating win rate for {tf_key}: {e}\")\n",
    "\n",
    "print(f\"\\n  Total Buys: {total_buys}\")\n",
    "print(f\"  Total Sells: {total_sells}\")\n",
    "print(f\"Total Fees Paid (Strategy): {total_fees_paid:.4f} {QUOTE_ASSET}\")\n",
    "print(f\"Grid Abandon Counts: {abandon_counts}\")\n",
    "\n",
    "# --- Plotting (Strategy vs HODL) ---\\\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    portfolio_df = pd.DataFrame(portfolio_history).set_index('timestamp')\n",
    "    hodl_df = pd.DataFrame(hodl_portfolio_history).set_index('timestamp')\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    color = 'tab:blue'; ax1.set_xlabel('Date'); ax1.set_ylabel('Portfolio Value (USDT)', color=color)\n",
    "    strat_line, = ax1.plot(portfolio_df.index, portfolio_df['portfolio_value'].astype(float), color=color, linewidth=1.5, label='Strategy Value (v9.0)') # Updated label\n",
    "    hodl_color = 'darkgrey'; hodl_line, = ax1.plot(hodl_df.index, hodl_df['hodl_value'].astype(float), color=hodl_color, linestyle='--', linewidth=1.0, label='HODL Value')\n",
    "    ax1.tick_params(axis='y', labelcolor=color);\n",
    "    min_val_strat = portfolio_df['portfolio_value'].astype(float).min() if not portfolio_df.empty else float(STARTING_QUOTE_BALANCE)\n",
    "    max_val_strat = portfolio_df['portfolio_value'].astype(float).max() if not portfolio_df.empty else float(STARTING_QUOTE_BALANCE)\n",
    "    min_val_hodl = hodl_df['hodl_value'].astype(float).min() if not hodl_df.empty else float(STARTING_QUOTE_BALANCE)\n",
    "    max_val_hodl = hodl_df['hodl_value'].astype(float).max() if not hodl_df.empty else float(STARTING_QUOTE_BALANCE)\n",
    "    min_val = min(min_val_strat, min_val_hodl); max_val = max(max_val_strat, max_val_hodl)\n",
    "    ax1.set_ylim(bottom=min_val * 0.95 if min_val > 0 else -100, top=max_val * 1.05 if max_val > 0 else 1100)\n",
    "    freq_str = getattr(historical_data_test.index, 'freqstr', None) or \"Hourly\"\n",
    "    fig.suptitle(f'Backtest v9.0: PL Filtered Dyn S/R Entry + Scaled PL% TP + Abandon - {SYMBOL} ({freq_str})') # Updated title\n",
    "    handles = [strat_line, hodl_line]; labels = [strat_line.get_label(), hodl_line.get_label()]; ax1.legend(handles, labels, loc='upper left')\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.show()\n",
    "except ImportError: logging.warning(\"matplotlib not installed. Skipping plot generation. `pip install matplotlib`\")\n",
    "except Exception as plot_err: logging.error(f\"Error during plotting: {plot_err}\")\n",
    "\n",
    "# End of Cell (Originally Cell 12, v9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3314b-2352-4b25-9417-f451e8393cdf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
